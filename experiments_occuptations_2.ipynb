{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from image_database import *\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "import bisect \n",
    "from scipy.spatial import ConvexHull\n",
    "import gc\n",
    "import debias_clip as dclip\n",
    "#import cvxpy as cp\n",
    "image_folder_prefix = 'datasets/occupations_2/images/'\n",
    "\n",
    "import sys\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "data = pd.read_csv('datasets/occupations_2/occupations_labels.csv') #datasets\\occuptations_2\\occupations_labels.csv\n",
    "extract_term = lambda x: x.split('/')[0]\n",
    "data['search_term'] = data['location'].apply(extract_term)\n",
    "\n",
    "catagories = data['search_term'].unique()\n",
    "define_path = lambda x: image_folder_prefix + x \n",
    "data['image_relative_path'] = data['location'].apply(define_path)\n",
    "\n",
    "flawed_images = [\"PR person/000056.jpg\", \"mail carrier/000067.jpg\"]\n",
    "\n",
    "data.drop(data[data['location'].isin(flawed_images)].index, inplace = True)\n",
    "\n",
    "true_rate_women = {}\n",
    "true_rate_dark = {}\n",
    "\n",
    "for cat in catagories:\n",
    "    data_in_cat = data[data['search_term'] == cat]\n",
    "    true_rate_women[cat] = data_in_cat[data_in_cat['gender'] == 'Female'].count()[0]/data_in_cat.count()[0]\n",
    "    true_rate_dark[cat] = data_in_cat[data_in_cat['skintone'] == 'dark'].count()[0]/data_in_cat.count()[0]\n",
    "    data.loc[data['search_term'] == cat, 'search_p_women'] = true_rate_women[cat]\n",
    "    data.loc[data['search_term'] == cat, 'search_p_dark'] = true_rate_dark[cat]\n",
    "\n",
    "print(len(catagories))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "if True:\n",
    "    features = process_images(model, preprocess, data['image_relative_path'], batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_d = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_debias, preprocess_debias = dclip.load(\"ViT-B/16-gender\", device_d)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "features_debias = process_images(model_debias, preprocess_debias, data['image_relative_path'], batch_size=batch_size, device=device_d)\n",
    "\n",
    "device_d = 'cpu'\n",
    "model_debias, preprocess_debias = dclip.load(\"ViT-B/16-gender\", device_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)\n",
    "\n",
    "np.save('datasets/occupations_2/embeddings_of_labelled_images.npy', features)\n",
    "np.save('datasets/occupations_2/embeddings_of_labelled_images_debias.npy', features_debias)\n",
    "\n",
    "if False:\n",
    "    features = np.load('datasets/occupations_2/embeddings_of_labelled_images.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('datasets/occupations_2/embeddings_of_labelled_images.npy')\n",
    "features_debias = np.load('datasets/occupations_2/embeddings_of_labelled_images_debias.npy')\n",
    "\n",
    "device_d = 'cpu'\n",
    "model_debias, preprocess_debias = dclip.load(\"ViT-B/16-gender\", device_d)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Written referencing https://stackoverflow.com/questions/30227466/combine-several-images-horizontally-with-python\n",
    "def process_images(image_addresses, max_size=(200, 200)):\n",
    "    images = [Image.open(image_address) for image_address in image_addresses]\n",
    "    for image in images:\n",
    "        image.thumbnail(max_size)\n",
    "    new_im = Image.new('RGB', (150 * 5, 300), color='white')\n",
    "    x_offset = 0\n",
    "    for i, im in enumerate(images):\n",
    "        b = Image.new('RGB', (150, 150), 'black')\n",
    "        if i <= len(images) // 2 - 1:\n",
    "            new_im.paste(b, (x_offset, 0))\n",
    "            new_im.paste(im, (x_offset, 0))\n",
    "        else:\n",
    "            new_im.paste(b, (x_offset, 150))\n",
    "            new_im.paste(im, (x_offset, 150))\n",
    "        x_offset += 150\n",
    "        if i == len(images) // 2 - 1:\n",
    "            x_offset = 0\n",
    "    return new_im\n",
    "\n",
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "image_database.sensitive_attributes([(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of a young person\", \"A picture of an old person\")])\n",
    "\n",
    "print(data.head())\n",
    "print(data['search_term'].unique())\n",
    "\n",
    "print(image_database.features.dtype)\n",
    "print(image_database.search(\"This is a picture of a pilot.\"))\n",
    "pilot1 = image_database.search(\"This is a picture of a pilot.\")\n",
    "pilot2 = image_database.distinct_retrival(\"This is a picture of a pilot.\", tol=.035)\n",
    "\n",
    "p1c = process_images(pilot1['image_relative_path'])\n",
    "p1c.save('results/graphics/pilot1.jpg')\n",
    "p2c = process_images(pilot2['image_relative_path'])\n",
    "p2c.save('results/graphics/pilot2.jpg')\n",
    "\n",
    "l1 = image_database.search(\"This is a picture of a librarian.\")\n",
    "l2 = image_database.distinct_retrival(\"This is a picture of a librarian.\", tol=.035)\n",
    "\n",
    "l1c = process_images(l1['image_relative_path'])\n",
    "l1c.save('results/graphics/librarian1.jpg')\n",
    "l2c = process_images(l2['image_relative_path'])\n",
    "l2c.save('results/graphics/librarian2.jpg')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDatabase():\n",
    "    def __init__(self, features, data, model, preprocess, device=\"cuda\"):\n",
    "        if device == \"cuda\": self.arraytype = torch.float16 \n",
    "        else: self.arraytype = torch.float32\n",
    "        self.features = t_normalize(torch.tensor(features).to(device)).to(self.arraytype)\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "        self.device = device\n",
    "        self.clipclip_orderings = {}\n",
    "\n",
    "    def _process_query(self, query):\n",
    "        token = clip.tokenize(query).to(self.device)\n",
    "        #print(token.device)\n",
    "        with torch.no_grad():\n",
    "            query_features = t_normalize(self.model.encode_text(token))\n",
    "        return query_features\n",
    "        \n",
    "    def search(self, query, k=10, **kwargs):\n",
    "        query_features = self._process_query(query)\n",
    "        similarities = (self.features @ query_features.T).flatten()\n",
    "        best = similarities.argsort(descending=True).cpu().flatten()\n",
    "        return self.data.iloc[best[:k]], similarities \n",
    "    \n",
    "    def sensitive_attributes(self, paired_attributes):\n",
    "        l_attr = list(sum(paired_attributes, ()))\n",
    "        tokens = clip.tokenize(l_attr).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            attributes_features = t_normalize(self.model.encode_text(tokens))\n",
    "        self.sensitive_ideals = attributes_features.reshape(len(paired_attributes), 2, -1)\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def _calc_sim_set(self, best, similarities, k, max_sim_set, tol):\n",
    "        sim = similarities[best]\n",
    "        sim_top_k_avg = sim[0:k].mean()\n",
    "        for i in range(len(sim)):\n",
    "            if sim[i] < sim_top_k_avg - tol:\n",
    "                break\n",
    "        k = min(max(k, i), max_sim_set)\n",
    "        return best[0:k]\n",
    "\n",
    "\n",
    "    def _get_sim_to_ideal(self, entries):\n",
    "        sim_set_features = self.features[entries]\n",
    "        sim = torch.matmul(self.sensitive_ideals, sim_set_features.T)\n",
    "        proba = (100 * sim).permute(2, 0, 1).softmax(dim=-1)[:, :, 0].to('cpu').numpy()\n",
    "        return proba\n",
    "\n",
    "    def _retrieve_distinct(self, sim_set, similarities, k, mode='max_sum'):\n",
    "        proba_concepts = self._get_sim_to_ideal(sim_set)\n",
    "        vals = proba_concepts\n",
    "        pca = PCA()\n",
    "        \n",
    "        if mode == 'max_sum': \n",
    "            VI=np.cov(vals, rowvar=False)\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'mahalanobis', VI=VI)\n",
    "\n",
    "            while len(p) < k:\n",
    "                mean_distances = np.mean(distances, axis=1)\n",
    "                max_sum = np.argmax(mean_distances)\n",
    "                maximally_away = max_sum\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'mahalanobis', VI=VI)))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "        \n",
    "        if mode == 'euc_max_sum': \n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'euclidean')\n",
    "\n",
    "            while len(p) < k:\n",
    "                mean_distances = np.mean(distances, axis=1)\n",
    "                max_sum = np.argmax(mean_distances)\n",
    "                maximally_away = max_sum\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'euclidean')))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "\n",
    "        if mode == 'max_min':\n",
    "            VI=np.cov(vals, rowvar=False)\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'mahalanobis', VI=VI)\n",
    "\n",
    "            while len(p) < k:\n",
    "                min_distances = np.min(distances, axis=1)\n",
    "                max_min = np.argmax(min_distances)\n",
    "                maximally_away = max_min\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'mahalanobis', VI=VI)))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "        \n",
    "        if mode == 'euc_max_min':\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'euclidean')\n",
    "\n",
    "            while len(p) < k:\n",
    "                min_distances = np.min(distances, axis=1)\n",
    "                max_min = np.argmax(min_distances)\n",
    "                maximally_away = max_min\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'euclidean')))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "\n",
    "        if mode == 'random':\n",
    "            p_indices = np.random.choice(sim_set, k, replace = False)\n",
    "            \n",
    "            \n",
    "        if mode == \"feature_distances\":\n",
    "            vals = torch.index_select(self.features, 0, sim_set.to(self.device)).cpu().numpy()\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'cosine')\n",
    "\n",
    "            while len(p) < k:\n",
    "                mean_distances = np.mean(distances, axis=1)\n",
    "                max_sum = np.argmax(mean_distances)\n",
    "                maximally_away = max_sum\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'cosine')))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "\n",
    "        if mode == 'true_labels':\n",
    "            vals = self._get_true_coordinates(sim_set)\n",
    "            VI=np.cov(vals, rowvar=False)\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'mahalanobis', VI=VI)\n",
    "\n",
    "            while len(p) < k:\n",
    "                mean_distances = np.mean(distances, axis=1)\n",
    "                max_sum = np.argmax(mean_distances)\n",
    "                maximally_away = max_sum\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'mahalanobis', VI=VI)))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "\n",
    "        return p_indices\n",
    "    \n",
    "    def define_coordinate_mapping(self, columns, positive_labels, negative_labels):\n",
    "        self.true_coordinates = np.zeros((len(self.data), len(columns)))\n",
    "        self.coord_columns = columns\n",
    "        for i, column in enumerate(columns):\n",
    "            map_to_hypercube = lambda x: 1 if x in positive_labels[i] else (0 if x in negative_labels[i] else .5)\n",
    "            self.true_coordinates[:, i] = self.data[column].apply(map_to_hypercube)\n",
    "    \n",
    "    def _get_true_coordinates(self, sim_set):\n",
    "        return self.true_coordinates[sim_set] \n",
    "\n",
    "    \n",
    "    def distinct_retrival(self, query, k=10, max_sim_set=1000, tol=.06, method='max_sum', **kwargs) :\n",
    "        query_features = self._process_query(query)\n",
    "\n",
    "        similarities = (self.features @ query_features.T).flatten()\n",
    "        best = similarities.argsort(descending=True).cpu().flatten()\n",
    "\n",
    "        sim_set = self._calc_sim_set(best, similarities, k, max_sim_set, tol)\n",
    "        distinct_sort = self._retrieve_distinct(sim_set, similarities, k, mode=method)\n",
    "\n",
    "        coords = self._get_sim_to_ideal(sim_set)\n",
    "    \n",
    "        return self.data.iloc[distinct_sort], coords, self._get_sim_to_ideal(distinct_sort), self._get_sim_to_ideal(best[0:k])\n",
    "\n",
    "    def define_pbm_classes(self, classes):\n",
    "        self.pbm_classes=classes\n",
    "        prompts = [f\"A picture of a {c}.\" for c in classes]\n",
    "        if classes[0] == \"empty\":\n",
    "            prompts[0] == \"\"\n",
    "        tokens = clip.tokenize(prompts).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            attributes_features = t_normalize(self.model.encode_text(tokens))\n",
    "        self.pbm_ideals = attributes_features\n",
    "        self.pbm_label = np.argmax((100 * torch.matmul(self.features, self.pbm_ideals.T)).softmax(dim=-1).to('cpu').numpy(), axis=-1)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def pbm(self, query, k=10, eps=0, **kwargs):\n",
    "        query_features = self._process_query(query)\n",
    "        similarities = (self.features @ query_features.T).flatten()\n",
    "        best = similarities.argsort(descending=True).cpu().numpy().flatten()\n",
    "        np_sim = similarities.cpu().numpy()\n",
    "\n",
    "        p_indices = []\n",
    "\n",
    "        neutrals = [x for x in best if self.pbm_label[x] == 0]\n",
    "        classes = [[x for x in best if self.pbm_label[x]== i] for i in range(1, len(self.pbm_classes))]\n",
    "\n",
    "    \n",
    "        while len(p_indices) < k:\n",
    "            if random.random() < eps:\n",
    "                try:\n",
    "                    neutral_sim = np_sim[neutrals[0]]\n",
    "                except:\n",
    "                    neutral_sim = -1\n",
    "                \n",
    "                max_class, idx = 0, 0\n",
    "                for i, c in enumerate(classes):\n",
    "                    try:\n",
    "                        class_sim = np_sim[c[0]]\n",
    "                    except:\n",
    "                        class_sim = -1\n",
    "                    if class_sim > max_class:\n",
    "                        max_class = class_sim\n",
    "                        idx = i\n",
    "                if max_class > neutral_sim:\n",
    "                    p_indices.append(classes[idx][0])\n",
    "                    classes[idx].pop(0)\n",
    "                else:\n",
    "                    p_indices.append(neutrals[0])\n",
    "                    neutrals.pop(0)\n",
    "                        \n",
    "            else:\n",
    "                best_neutral = neutrals[0]\n",
    "                best_for_classes = [fon(c) for c in classes]\n",
    "                best_for_classes_vals = [c for c in best_for_classes if c is not None]\n",
    "\n",
    "                similarities_for_classes = [np_sim[x] for x in best_for_classes_vals]\n",
    "                avg_sim = np.mean(similarities_for_classes)\n",
    "                neutral_sim = similarities[best_neutral]\n",
    "\n",
    "                if avg_sim > neutral_sim:\n",
    "                    if len(p_indices) + len(best_for_classes_vals) > k:\n",
    "                        best_for_classes_vals = random.choices(best_for_classes_vals, k=k-len(p_indices))\n",
    "                    p_indices += best_for_classes_vals\n",
    "\n",
    "                    for i, x in enumerate(best_for_classes):\n",
    "                        if x is not None:\n",
    "                            classes[i].pop(0)\n",
    "                else:\n",
    "                    p_indices.append(best_neutral)\n",
    "                    neutrals.pop(0)\n",
    "        \n",
    "        return self.data.iloc[p_indices]\n",
    "\n",
    "    def add_clipclip_ordering(self, name, ordering):\n",
    "        self.clipclip_orderings[name] = ordering.copy()\n",
    "        return self\n",
    "    \n",
    "    def clip_clip(self, query, ordering, n_to_clip, k=10, **kwargs):\n",
    "        query_features = self._process_query(query)\n",
    "        clip_ordering = self.clipclip_orderings[ordering]\n",
    "        clip_features = torch.index_select(self.features, 1, torch.tensor(clip_ordering[n_to_clip:]).to(self.device))\n",
    "        clip_query = torch.index_select(query_features, 1, torch.tensor(clip_ordering[n_to_clip:]).to(self.device))\n",
    "\n",
    "        similarities = (clip_features @ clip_query.T).flatten()\n",
    "        best = similarities.argsort(descending=True).cpu().flatten()\n",
    "        return self.data.iloc[best[:k]]\n",
    "    \n",
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "image_database.sensitive_attributes([(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])\n",
    "\n",
    "res, sim = image_database.search(\"This is a picture of a Dentist.\", k=25)\n",
    "hp = sim.sort(descending=True).values[0:25].mean().cpu().numpy()\n",
    "print(hp)\n",
    "\n",
    "plt.hist(sim.cpu().numpy(), bins=50)\n",
    "plt.axvline(x=hp, color='red', linestyle='--')\n",
    "plt.axvline(x=hp-.02, color='green', linestyle='--')\n",
    "plt.xlabel('Similarity', weight='bold')\n",
    "plt.ylabel('Frequency', weight='bold')\n",
    "plt.title('Histogram of Similarities for Dentist Query', weight='bold')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "res, coords, P, tk = image_database.distinct_retrival(\"This is a picture of a Dentist.\", tol=.02, k=25, method='max_sum')\n",
    "\n",
    "plt.scatter(coords[:, 0], coords[:, 1], label='Images in Sim Set')\n",
    "plt.scatter(P[:,0], P[:, 1], label='Returned Images')\n",
    "#plt.scatter(tk[:,0], tk[:, 1], label='Top K Images', alpha=.3)\n",
    "plt.xlabel(\"Gender\", weight='bold')\n",
    "plt.ylabel(\"Skin-Tone\", weight='bold')\n",
    "plt.title(\"Plot of Concept Space for Dentist Query\", weight='bold')\n",
    "plt.grid()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "indistinguisable_values = [[cat] for cat in catagories]\n",
    "totals_by_cat = {cat: len(data[data['search_term'] == cat]) for cat in catagories}\n",
    "true_rates = [[data[data['search_term'] == cat].iloc[0].search_p_women for cat in catagories], [data[data['search_term'] == cat].iloc[0].search_p_dark for cat in catagories]]\n",
    "image_database.define_coordinate_mapping(['gender', 'skintone'], [['man'], ['light']], [['woman'], ['dark']])\n",
    "\n",
    "debias_database = ImageDatabase(features_debias, data, model_debias, preprocess_debias, device_d)\n",
    "\n",
    "image_database.add_clipclip_ordering(\"gender\", np.load('datasets/MI_orders/gender.npy'))\n",
    "image_database.add_clipclip_ordering(\"skintone\", np.load('datasets/MI_orders/skintone.npy'))\n",
    "image_database.add_clipclip_ordering(\"intersectional\", np.load('datasets/MI_orders/gender_skintone.npy'))\n",
    "image_database.add_clipclip_ordering(\"three_attr\", np.load('datasets/MI_orders/intersectional.npy'))\n",
    "\n",
    "\n",
    "method_name_specification_list = [\n",
    "    (lambda k, tol: lambda x: image_database.search(x, k), 'Baseline', []),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_gender', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_skintone', [(\"sensitive_attributes\", [(\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_3_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_4_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\"), (\"A picture of an American person\", \"A picture of a non-American person\")])]),\n",
    "\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_gender', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_skintone', [(\"sensitive_attributes\", [(\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_3_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_4_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\"), (\"A picture of an American person\", \"A picture of a non-American person\")])]),\n",
    "\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='random'), 'CDI_Random', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='feature_distances'), 'CDI_Features', []),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='true_labels'), 'CDI_TrueConcept', []),\n",
    "    \n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_gender', [(\"pbm_classes\", [\"unknown gender\", \"man\", \"woman\"])]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_intersectional', [(\"pbm_classes\", [\"unknown gender and skin-tone\", \"light-skinned man\", \"light-skinned woman\", \"dark-skinned man\", \"dark-skinned woman\"])]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_skintone', [(\"pbm_classes\", [\"unknown skin-tone\", \"light-skinned person\", \"dark-skinned person\"])]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_three_attributes', [(\"pbm_classes\", intersections_for_pbm([[\"light-skinned\", \"dark-skinned\"], [\"old\", \"young\"], [\"man\", \"woman\"]], \"an unknown skin-tone, age, and gender\", \"\"))]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_four_attributes', [(\"pbm_classes\", intersections_for_pbm([[\"light-skinned\", \"dark-skinned\"], [\"old\", \"young\"], [\"American\", \"non-American\"], [\"man\", \"woman\"]], \"an unknown skin-tone, age, nationality, and gender\", \"\"))]),\n",
    "    \n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"gender\", n, k), 'CLIP_gender', []),\n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"skintone\", n, k), 'CLIP_skintone', []),\n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"intersectional\", n, k), 'CLIP_intersectional', []),\n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"three_attr\", n, k), 'CLIP_three_attributes', []),\n",
    "\n",
    "    (lambda k, tol: lambda x: debias_database.search(x, k), 'DebiasClip', []),\n",
    "\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_sum'), 'CDI_EucSum_3_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_sum'), 'CDI_EucSum_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_min'), 'CDI_EucMin_3_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_min'), 'CDI_EucMin_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])])\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "all_sensitive_pairs = [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\"), (\"A picture of an American person\", \"A picture of a non-American person\"), (\"A picture of a abled-bodied person\", \"A picture of a disabled person\"), (\"A picture of a religious person\", \"A picture of a non-religious person\"), (\"A picture of a veteran\", \"A picture of someone who is not a veteran\"), (\"A picture of a straight person\", \"A picture of a LGBT person\")]\n",
    "overconcepted_pairs = [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of someone masculine\", \"A picture of someone feminine\"), (\"A picture of a male\", \"A picture of a female\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")]\n",
    "\n",
    "method_name_specification_list = [\n",
    "    (lambda k, tol: lambda x: image_database.search(x, k), 'Baseline', []),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_3_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_6_attr', [(\"sensitive_attributes\", all_sensitive_pairs[:6])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_8_attr', [(\"sensitive_attributes\", all_sensitive_pairs)]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_3_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_6_attr', [(\"sensitive_attributes\", all_sensitive_pairs[:6])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_8_attr', [(\"sensitive_attributes\", all_sensitive_pairs)]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_min'), 'CDI_EucMin_3_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_min'), 'CDI_EucMin_6_attr', [(\"sensitive_attributes\", all_sensitive_pairs[:6])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_min'), 'CDI_EucMin_8_attr', [(\"sensitive_attributes\", all_sensitive_pairs)]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_sum'), 'CDI_EucSum_3_attr', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of an old person\", \"A picture of a young person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_sum'), 'CDI_EucSum_6_attr', [(\"sensitive_attributes\", all_sensitive_pairs[:6])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_sum'), 'CDI_EucSum_8_attr', [(\"sensitive_attributes\", all_sensitive_pairs)]),\n",
    "\n",
    "\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_overconcept', [(\"sensitive_attributes\", overconcepted_pairs)]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_overconcept', [(\"sensitive_attributes\", overconcepted_pairs)]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_min'), 'CDI_EucMin_overconcept', [(\"sensitive_attributes\", overconcepted_pairs)]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_sum'), 'CDI_EucSum_overconcept', [(\"sensitive_attributes\", overconcepted_pairs)]),\n",
    "\n",
    "]\n",
    "\n",
    "method_name_specification_list = [\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_five_attributes', [(\"pbm_classes\", intersections_for_pbm([[\"light-skinned\", \"dark-skinned\"], [\"old\", \"young\"], [\"American\", \"non-American\"], [\"abled-bodied\", \"disabled\"], [\"man\", \"woman\"]], \"an unknown skin-tone, age, nationality, disability status, and gender\", \"\"))]),\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "ks = [10, 25, 50, 100] # [10, 25, 50, 100] \n",
    "\n",
    "number_of_tol_steps = 16 # 16\n",
    "number_of_eps_steps = 11 # 11\n",
    "number_of_clip_clip_steps = 24 #24\n",
    "random_iters = 4\n",
    "\n",
    "result_dicts = []\n",
    "\n",
    "for method, name, spec in method_name_specification_list:\n",
    "    print(f\"Starting analysis for method: {name}...\")\n",
    "    for s, val in spec:\n",
    "        if s == \"sensitive_attributes\":\n",
    "            image_database.sensitive_attributes(val)\n",
    "        if s == \"pbm_classes\":\n",
    "            image_database.define_pbm_classes(val)\n",
    "    for k in tqdm(ks):\n",
    "        result_dict = {'name': name}\n",
    "\n",
    "        if name in ['Baseline', \"DebiasClip\"]:\n",
    "            steps = 1\n",
    "        else:\n",
    "            steps = number_of_tol_steps\n",
    "\n",
    "        if name[0:3] == 'PBM':\n",
    "            for e in reversed(range(0, number_of_eps_steps)):\n",
    "                eps = e / (number_of_eps_steps - 1)\n",
    "                retrieval_function = method(k, eps)\n",
    "                new_dict = result_dict.copy()\n",
    "                random_results = []\n",
    "                for i in range(random_iters):\n",
    "                    new_dict = result_dict.copy()\n",
    "                    run_analysis(retrieval_function, k, eps, new_dict, catagories, 'search_term', indistinguisable_values, ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "                    random_results.append(new_dict)\n",
    "                \n",
    "                add_dict = result_dict.copy()\n",
    "                for key in random_results[0].keys():\n",
    "                    if key == 'name':\n",
    "                        continue\n",
    "                    add_dict[key] = np.mean([res[key] for res in random_results], axis=0)\n",
    "                result_dicts.append(add_dict)\n",
    "\n",
    "        elif name[0:4] == 'CLIP':\n",
    "            for e in range(0, number_of_clip_clip_steps):\n",
    "                n = e * 20\n",
    "                retrieval_function = method(k, n)\n",
    "                new_dict = result_dict.copy()\n",
    "                run_analysis(retrieval_function, k, n, new_dict, catagories, 'search_term', indistinguisable_values, ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "                result_dicts.append(new_dict)\n",
    "        else:\n",
    "            for t in range(0, steps):\n",
    "                if steps == 1:\n",
    "                    tol = None\n",
    "                else:\n",
    "                    tol = t / 200\n",
    "                retrieval_function = method(k, tol)\n",
    "                new_dict = result_dict.copy()\n",
    "                if name == \"CDI_Random\":\n",
    "                    random_results = []\n",
    "                    for i in range(random_iters):\n",
    "                        new_dict = result_dict.copy()\n",
    "                        run_analysis(retrieval_function, k, tol, new_dict, catagories, 'search_term', indistinguisable_values,  ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "                        random_results.append(new_dict)\n",
    "                    new_dict = result_dict.copy()\n",
    "\n",
    "                    for key in random_results[0].keys():\n",
    "                        if key == 'name':\n",
    "                            continue\n",
    "                        new_dict[key] = np.mean([res[key] for res in random_results], axis=0)\n",
    "                else:\n",
    "                    run_analysis(retrieval_function, k, tol, new_dict, catagories, 'search_term', indistinguisable_values, ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "                result_dicts.append(new_dict)\n",
    "\n",
    "df = parse_analysis(result_dicts, ['gender', 'skintone'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_pickle(\"results/occ2-pbm-five-concepts.pkl\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_baseline = {}\n",
    "d_baseline['name'] = 'Baseline'\n",
    "d_maxsum = {}\n",
    "d_maxsum['name'] = 'CDI_MaxMin'\n",
    "\n",
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "image_database.sensitive_attributes([(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])\n",
    "\n",
    "run_analysis(lambda x: image_database.search(x, 50), 50, None, d_baseline, catagories, 'search_term', indistinguisable_values, ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 50, tol=.03, method='max_min'), 50, .03, d_maxsum, catagories, 'search_term', indistinguisable_values, ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_maxsum['precision'].mean(), d_baseline['precision'].mean())\n",
    "\n",
    "plt.scatter(d_baseline['bias'][:,0], d_baseline['precision'], alpha=.8)\n",
    "plt.scatter(d_maxsum['bias'][:,0], d_maxsum['precision'], alpha=.8)\n",
    "plt.title(\"Bias vs Precision on Occupations\", weight='bold')\n",
    "plt.xlabel(\"Gender Bias\", weight='bold', fontsize=12)\n",
    "plt.ylabel(\"Precision\", weight='bold', fontsize=12)\n",
    "plt.legend([\"Baseline\", \"CDI-MaxMin, tol = .03\"], loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names = df['name'].unique().tolist()\n",
    "print(df)\n",
    "df2 = pd.read_pickle(\"results/occ2-3-23.pkl\")\n",
    "print(df2)\n",
    "df3 = pd.concat([df2, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3)\n",
    "method_names += ['CDI_Sum_intersectional', 'CDI_Min_intersectional', 'CDI_Sum_3_attr', 'CDI_Min_3_attr']\n",
    "\n",
    "for k in [10, 25, 50, 100]:\n",
    "    plot_across_tol(df3, k, method_names, 'Avg_AbsBias_gender', 'Avg_Precision', reverse_x=True)\n",
    "    plot_across_tol(df3, k, method_names, 'Avg_Bias_gender', 'Avg_Precision', reverse_x=False)\n",
    "    plot_across_tol(df3, k, method_names, 'Avg_Skew_gender', 'Avg_Precision', reverse_x=False)\n",
    "    plot_across_tol(df3, k, method_names, 'Avg_AbsBias_gender', 'Avg_Recall', reverse_x=True)\n",
    "    plot_across_tol(df3, k, method_names, 'Avg_Bias_gender', 'Avg_Recall', reverse_x=False)\n",
    "    plot_across_tol(df3, k, method_names, 'Avg_AbsBias_for_Accurate_gender', 'Avg_Precision', reverse_x=True)\n",
    "    plot_across_tol(df3, k, method_names, 'Avg_Max_MC_Bias', 'Avg_Precision', reverse_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(totals_by_cat)\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "for cat in catagories:\n",
    "    res = image_database.clip_clip(f\"This is a picture of a {cat}\", \"intersectional\", 100, k=100)\n",
    "    recalls.append(recall(label_column='search_term', positive_label_value=cat, data=res, total_positive=totals_by_cat[cat]))\n",
    "    precisions.append(precision(label_column='search_term', positive_label_value=cat, data=res))\n",
    "\n",
    "plt.scatter(recalls, precisions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name_specification_list = [\n",
    "    (lambda k, tol: lambda x: image_database.search(x, k), 'Baseline', []),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_intersectional', [(\"pbm_classes\", [\"unknown gender and skin-tone\", \"light-skinned man\", \"light-skinned woman\", \"dark-skinned man\", \"dark-skinned woman\"])]),\n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"intersectional\", n, k), 'CLIP_intersectional', []),\n",
    "    (lambda k, tol: lambda x: debias_database.search(x, k), 'DebiasClip', [])\n",
    "]\n",
    "np.random.seed(123)\n",
    "result_dicts = []\n",
    "for runs in range(5):\n",
    "    r_indexs = np.random.choice(len(data), int(len(data) * .7), replace=False)\n",
    "    f_random = features[r_indexs]\n",
    "    d_random = data.iloc[r_indexs]\n",
    "    fd_random = features_debias[r_indexs]\n",
    "    image_database = ImageDatabase(f_random, d_random, model, preprocess, device)\n",
    "    indistinguisable_values = [[cat] for cat in catagories]\n",
    "    totals_by_cat = {cat: len(data[data['search_term'] == cat]) for cat in catagories}\n",
    "    true_rates = [[data[data['search_term'] == cat].iloc[0].search_p_women for cat in catagories], [data[data['search_term'] == cat].iloc[0].search_p_dark for cat in catagories]]\n",
    "    image_database.define_coordinate_mapping(['gender', 'skintone'], [['man'], ['light']], [['woman'], ['dark']])\n",
    "    \n",
    "    debias_database = ImageDatabase(fd_random, d_random, model_debias, preprocess_debias, device_d)\n",
    "    image_database.add_clipclip_ordering(\"intersectional\", np.load('datasets/MI_orders/gender_skintone.npy'))\n",
    "\n",
    "    ks = [10, 25, 50, 100] # [10, 25, 50, 100] \n",
    "\n",
    "    number_of_tol_steps = 16 # 16\n",
    "    number_of_eps_steps = 11 # 11\n",
    "    number_of_clip_clip_steps = 24 #24\n",
    "    random_iters = 4\n",
    "\n",
    "    for method, name, spec in method_name_specification_list:\n",
    "        print(f\"Starting analysis for method: {name}...\")\n",
    "        for s, val in spec:\n",
    "            if s == \"sensitive_attributes\":\n",
    "                image_database.sensitive_attributes(val)\n",
    "            if s == \"pbm_classes\":\n",
    "                image_database.define_pbm_classes(val)\n",
    "        for k in tqdm(ks):\n",
    "            result_dict = {'name': name}\n",
    "\n",
    "            if name in ['Baseline', \"DebiasClip\"]:\n",
    "                steps = 1\n",
    "            else:\n",
    "                steps = number_of_tol_steps\n",
    "\n",
    "            if name[0:3] == 'PBM':\n",
    "                for e in reversed(range(0, number_of_eps_steps)):\n",
    "                    eps = e / (number_of_eps_steps - 1)\n",
    "                    retrieval_function = method(k, eps)\n",
    "                    new_dict = result_dict.copy()\n",
    "                    random_results = []\n",
    "                    for i in range(random_iters):\n",
    "                        new_dict = result_dict.copy()\n",
    "                        run_analysis(retrieval_function, k, eps, new_dict, catagories, 'search_term', indistinguisable_values, ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "                        random_results.append(new_dict)\n",
    "                    \n",
    "                    add_dict = result_dict.copy()\n",
    "                    for key in random_results[0].keys():\n",
    "                        if key == 'name':\n",
    "                            continue\n",
    "                        add_dict[key] = np.mean([res[key] for res in random_results], axis=0)\n",
    "                    result_dicts.append(add_dict)\n",
    "\n",
    "            elif name[0:4] == 'CLIP':\n",
    "                for e in range(0, number_of_clip_clip_steps):\n",
    "                    n = e * 20\n",
    "                    retrieval_function = method(k, n)\n",
    "                    new_dict = result_dict.copy()\n",
    "                    run_analysis(retrieval_function, k, n, new_dict, catagories, 'search_term', indistinguisable_values, ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "                    result_dicts.append(new_dict)\n",
    "            else:\n",
    "                for t in range(0, steps):\n",
    "                    if steps == 1:\n",
    "                        tol = None\n",
    "                    else:\n",
    "                        tol = t / 200\n",
    "                    retrieval_function = method(k, tol)\n",
    "                    new_dict = result_dict.copy()\n",
    "                    if name == \"CDI_Random\":\n",
    "                        random_results = []\n",
    "                        for i in range(random_iters):\n",
    "                            new_dict = result_dict.copy()\n",
    "                            run_analysis(retrieval_function, k, tol, new_dict, catagories, 'search_term', indistinguisable_values,  ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "                            random_results.append(new_dict)\n",
    "                        new_dict = result_dict.copy()\n",
    "\n",
    "                        for key in random_results[0].keys():\n",
    "                            if key == 'name':\n",
    "                                continue\n",
    "                            new_dict[key] = np.mean([res[key] for res in random_results], axis=0)\n",
    "                    else:\n",
    "                        run_analysis(retrieval_function, k, tol, new_dict, catagories, 'search_term', indistinguisable_values, ['gender', 'skintone'], [['Female'], ['dark']], [['Male'], ['light']], true_rates, totals_by_cat)\n",
    "                    result_dicts.append(new_dict)\n",
    "\n",
    "df = parse_analysis(result_dicts, ['gender', 'skintone'])\n",
    "\n",
    "df.to_pickle(\"results/occ2-3-29-validation.pkl\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
