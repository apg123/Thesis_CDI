{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "import bisect \n",
    "from scipy.spatial import ConvexHull\n",
    "import gc\n",
    "import debias_clip as dclip\n",
    "import copy\n",
    "import sklearn.feature_selection as fs\n",
    "from image_database import *\n",
    "#import cvxpy as cp\n",
    "image_folder_prefix = 'datasets/gender_in_image_search/google/'\n",
    "\n",
    "data = pd.read_csv('datasets/gender_in_image_search/gender_labelled_images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "(3262, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alexa\\AppData\\Local\\Temp\\ipykernel_24304\\2415674966.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['image_relative_path'] = data['image_url'].str.replace(\"http://homes.cs.washington.edu/~mjskay/aws-images/google/\", image_folder_prefix)\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing\n",
    "\n",
    "catagories = data['search_term'].unique()\n",
    "data['image_relative_path'] = data['image_url'].str.replace(\"http://homes.cs.washington.edu/~mjskay/aws-images/google/\", image_folder_prefix)\n",
    "\n",
    "print(len(catagories))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:09<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "features = process_images(model, preprocess, data['image_relative_path'], batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_d = 'cpu'\n",
    "model_debias, preprocess_debias = dclip.load(\"ViT-B/16-gender\", device_d)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "features_debias = process_images(model_debias, preprocess_debias, data['image_relative_path'], batch_size=batch_size, device=device_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pretrained embedings\n",
      " best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-317_model_e4_step_5334_embeddings.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4.73k/4.73k [00:00<?, ?iB/s]\n",
      "100%|██████████| 17/17 [00:14<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pretrained embedings\n",
      " best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-317_model_e4_step_5334_embeddings.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4.73k/4.73k [00:00<?, ?iB/s]\n"
     ]
    }
   ],
   "source": [
    "device_d = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_debias, preprocess_debias = dclip.load(\"ViT-B/16-gender\", device_d)\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "features_debias = process_images(model_debias, preprocess_debias, data['image_relative_path'], batch_size=batch_size, device=device_d)\n",
    "\n",
    "device_d = 'cpu'\n",
    "model_debias, preprocess_debias = dclip.load(\"ViT-B/16-gender\", device_d)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class ImageDatabase():\n",
    "    def __init__(self, features, data, model, preprocess, device=\"cuda\"):\n",
    "        if device == \"cuda\": self.arraytype = torch.float16 \n",
    "        else: self.arraytype = torch.float32\n",
    "        self.features = t_normalize(torch.tensor(features).to(device)).to(self.arraytype)\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "        self.device = device\n",
    "        self.clipclip_orderings = {}\n",
    "\n",
    "    def _process_query(self, query):\n",
    "        token = clip.tokenize(query).to(self.device)\n",
    "        #print(token.device)\n",
    "        with torch.no_grad():\n",
    "            query_features = t_normalize(self.model.encode_text(token))\n",
    "        return query_features\n",
    "        \n",
    "    def search(self, query, k=10, **kwargs):\n",
    "        query_features = self._process_query(query)\n",
    "        similarities = (self.features @ query_features.T).flatten()\n",
    "        best = similarities.argsort(descending=True).cpu().flatten()\n",
    "        return self.data.iloc[best[:k]]\n",
    "    \n",
    "    def sensitive_attributes(self, paired_attributes):\n",
    "        l_attr = list(sum(paired_attributes, ()))\n",
    "        tokens = clip.tokenize(l_attr).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            attributes_features = t_normalize(self.model.encode_text(tokens))\n",
    "        self.sensitive_ideals = attributes_features.reshape(len(paired_attributes), 2, -1)\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def _calc_sim_set(self, best, similarities, k, max_sim_set, tol):\n",
    "        sim = similarities[best]\n",
    "        sim_top_k_avg = sim[0:k].mean()\n",
    "        for i in range(len(sim)):\n",
    "            if sim[i] < sim_top_k_avg - tol:\n",
    "                break\n",
    "        k = min(max(k, i), max_sim_set)\n",
    "        return best[0:k]\n",
    "\n",
    "\n",
    "    def _get_sim_to_ideal(self, entries):\n",
    "        sim_set_features = self.features[entries]\n",
    "        sim = torch.matmul(self.sensitive_ideals, sim_set_features.T)\n",
    "        proba = (100 * sim).permute(2, 0, 1).softmax(dim=-1)[:, :, 0].to('cpu').numpy()\n",
    "        return proba\n",
    "\n",
    "    def _retrieve_distinct(self, sim_set, similarities, k, mode='max_sum'):\n",
    "        proba_concepts = self._get_sim_to_ideal(sim_set)\n",
    "        vals = proba_concepts\n",
    "        pca = PCA()\n",
    "        \n",
    "        if mode == 'max_sum': \n",
    "            VI=np.cov(vals, rowvar=False)\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'mahalanobis', VI=VI)\n",
    "\n",
    "            while len(p) < k:\n",
    "                mean_distances = np.mean(distances, axis=1)\n",
    "                max_sum = np.argmax(mean_distances)\n",
    "                maximally_away = max_sum\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'mahalanobis', VI=VI)))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "\n",
    "        if mode == 'max_min':\n",
    "            VI=np.cov(vals, rowvar=False)\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'mahalanobis', VI=VI)\n",
    "\n",
    "            while len(p) < k:\n",
    "                min_distances = np.min(distances, axis=1)\n",
    "                max_min = np.argmax(min_distances)\n",
    "                maximally_away = max_min\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'mahalanobis', VI=VI)))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "\n",
    "        if mode == 'random':\n",
    "            p_indices = np.random.choice(sim_set, k, replace = False)\n",
    "\n",
    "        if mode == \"feature_distances\":\n",
    "            vals = torch.index_select(self.features, 0, sim_set.to(self.device)).cpu().numpy()\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'cosine')\n",
    "\n",
    "            while len(p) < k:\n",
    "                mean_distances = np.mean(distances, axis=1)\n",
    "                max_sum = np.argmax(mean_distances)\n",
    "                maximally_away = max_sum\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'cosine')))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "\n",
    "        if mode == 'true_labels':\n",
    "            vals = self._get_true_coordinates(sim_set)\n",
    "            VI=np.cov(vals, rowvar=False)\n",
    "            sim_set_edit = sim_set.detach().clone().numpy()\n",
    "            p_indices = [sim_set_edit[0]]\n",
    "            sim_set_edit = np.delete(sim_set_edit, 0)\n",
    "            p = [vals[0]]\n",
    "            vals = np.delete(vals, 0, axis=0)\n",
    "\n",
    "            distances = cdist(vals, p, 'mahalanobis', VI=VI)\n",
    "\n",
    "            while len(p) < k:\n",
    "                mean_distances = np.mean(distances, axis=1)\n",
    "                max_sum = np.argmax(mean_distances)\n",
    "                maximally_away = max_sum\n",
    "                p_indices.append(sim_set_edit[maximally_away])\n",
    "                p.append(vals[maximally_away])\n",
    "                distances = np.hstack((distances, cdist(vals, [vals[maximally_away]], 'mahalanobis', VI=VI)))\n",
    "                sim_set_edit = np.delete(sim_set_edit, maximally_away)\n",
    "                vals = np.delete(vals, maximally_away, axis=0)\n",
    "                distances = np.delete(distances, maximally_away, axis=0)\n",
    "\n",
    "        return p_indices\n",
    "    \n",
    "    def define_coordinate_mapping(self, columns, positive_labels, negative_labels):\n",
    "        self.true_coordinates = np.zeros((len(self.data), len(columns)))\n",
    "        self.coord_columns = columns\n",
    "        for i, column in enumerate(columns):\n",
    "            map_to_hypercube = lambda x: 1 if x in positive_labels[i] else (0 if x in negative_labels[i] else .5)\n",
    "            self.true_coordinates[:, i] = self.data[column].apply(map_to_hypercube)\n",
    "    \n",
    "    def _get_true_coordinates(self, sim_set):\n",
    "        return self.true_coordinates[sim_set] \n",
    "\n",
    "    \n",
    "    def distinct_retrival(self, query, k=10, max_sim_set=1000, tol=.06, method='max_sum', **kwargs) :\n",
    "        query_features = self._process_query(query)\n",
    "\n",
    "        similarities = (self.features @ query_features.T).flatten()\n",
    "        best = similarities.argsort(descending=True).cpu().flatten()\n",
    "\n",
    "        sim_set = self._calc_sim_set(best, similarities, k, max_sim_set, tol)\n",
    "        distinct_sort = self._retrieve_distinct(sim_set, similarities, k, mode=method)\n",
    "\n",
    "\n",
    "        return self.data.iloc[distinct_sort]\n",
    "\n",
    "    def define_pbm_classes(self, classes):\n",
    "        self.pbm_classes=classes\n",
    "        prompts = [f\"A picture of a {c}.\" for c in classes]\n",
    "        if classes[0] == \"empty\":\n",
    "            prompts[0] == \"\"\n",
    "        tokens = clip.tokenize(prompts).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            attributes_features = t_normalize(self.model.encode_text(tokens))\n",
    "        self.pbm_ideals = attributes_features\n",
    "        self.pbm_label = np.argmax((100 * torch.matmul(self.features, self.pbm_ideals.T)).softmax(dim=-1).to('cpu').numpy(), axis=-1)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def pbm(self, query, k=10, eps=0, **kwargs):\n",
    "        query_features = self._process_query(query)\n",
    "        similarities = (self.features @ query_features.T).flatten()\n",
    "        best = similarities.argsort(descending=True).cpu().numpy().flatten()\n",
    "        np_sim = similarities.cpu().numpy()\n",
    "\n",
    "        p_indices = []\n",
    "\n",
    "        neutrals = [x for x in best if self.pbm_label[x] == 0]\n",
    "        classes = [[x for x in best if self.pbm_label[x]== i] for i in range(1, len(self.pbm_classes))]\n",
    "\n",
    "        \n",
    "\n",
    "        while len(p_indices) < k:\n",
    "            if random.random() < eps:\n",
    "                try:\n",
    "                    neutral_sim = np_sim[neutrals[0]]\n",
    "                except:\n",
    "                    neutral_sim = -1\n",
    "                \n",
    "                max_class, idx = 0, 0\n",
    "                for i, c in enumerate(classes):\n",
    "                    try:\n",
    "                        class_sim = np_sim[c[0]]\n",
    "                    except:\n",
    "                        class_sim = -1\n",
    "                    if class_sim > max_class:\n",
    "                        max_class = class_sim\n",
    "                        idx = i\n",
    "                if max_class > neutral_sim:\n",
    "                    p_indices.append(classes[idx][0])\n",
    "                    classes[idx].pop(0)\n",
    "                else:\n",
    "                    p_indices.append(neutrals[0])\n",
    "                    neutrals.pop(0)\n",
    "            else:\n",
    "                best_neutral = neutrals[0]\n",
    "                best_for_classes = [fon(c) for c in classes]\n",
    "                best_for_classes_vals = [c for c in best_for_classes if c is not None]\n",
    "\n",
    "                similarities_for_classes = [np_sim[x] for x in best_for_classes_vals]\n",
    "                avg_sim = np.mean(similarities_for_classes)\n",
    "                neutral_sim = similarities[best_neutral]\n",
    "\n",
    "                if avg_sim > neutral_sim:\n",
    "                    if len(p_indices) + len(best_for_classes_vals) > k:\n",
    "                        best_for_classes_vals = random.choices(best_for_classes_vals, k=k-len(p_indices))\n",
    "                    p_indices += best_for_classes_vals\n",
    "\n",
    "                    for i, x in enumerate(best_for_classes):\n",
    "                        if x is not None:\n",
    "                            classes[i].pop(0)\n",
    "                else:\n",
    "                    p_indices.append(best_neutral)\n",
    "                    neutrals.pop(0)\n",
    "        \n",
    "        return self.data.iloc[p_indices]\n",
    "    \n",
    "    def add_clipclip_ordering(self, name, ordering):\n",
    "        self.clipclip_orderings[name] = ordering.copy()\n",
    "        return self\n",
    "    \n",
    "    def clip_clip(self, query, ordering, n_to_clip, k=10, **kwargs):\n",
    "        query_features = self._process_query(query)\n",
    "        clip_ordering = self.clipclip_orderings[ordering]\n",
    "        print(clip_ordering[n_to_clip:])\n",
    "        clip_features = torch.index_select(self.features, 1, torch.tensor(clip_ordering[n_to_clip:]).to(self.device))\n",
    "        print(clip_features.shape)\n",
    "        clip_query = torch.index_select(query_features, 1, torch.tensor(clip_ordering[n_to_clip:]).to(self.device))\n",
    "        print(clip_query.shape)\n",
    "\n",
    "        similarities = (clip_features @ clip_query.T).flatten()\n",
    "        print(similarities.shape)\n",
    "        best = similarities.argsort(descending=True).cpu().flatten()\n",
    "        return self.data.iloc[best[:k]]\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "image_database.sensitive_attributes([(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of a young person\", \"A picture of an old person\")])\n",
    "\n",
    "#print(image_database.search(\"This is a picture of a engineer.\"))\n",
    "print(image_database.distinct_retrival(\"This is a picture of a pilot.\", method='feature_distances', k=10))\n",
    "\n",
    "#image_database.define_pbm_classes([\"unknown gender\", \"man\", \"woman\"])\n",
    "#print(image_database.pbm(\"This is a picture of a customer service representative.\", eps=0))\n",
    "\n",
    "#debias_database = ImageDatabase(features_debias, data, model_debias, preprocess_debias, device_d)\n",
    "#debias_database.search(\"This is a picture of a engineer.\")\n",
    "\n",
    "\n",
    "#image_database.define_coordinate_mapping(['image_gender'], [['man']], [['woman']])\n",
    "#print(image_database.true_coordinates)\n",
    "#print(image_database.data['image_gender'])\n",
    "#print(image_database.distinct_retrival(\"This is a picture of a pilot.\", method='true_labels'))\n",
    "\n",
    "\n",
    "\n",
    "#image_database.add_clipclip_ordering(\"gender\", ord)\n",
    "#image_database.clip_clip(\"This is a picture of a pilot\", \"gender\", 350, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ord.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           method                name   tol   k  \\\n",
      "0                        Baseline            Baseline   NaN  25   \n",
      "1        DivImageMSum (tol: 0.02)        DivImageMSum  0.02  25   \n",
      "2        DivImageMMin (tol: 0.02)        DivImageMMin  0.02  25   \n",
      "3            RandomSS (tol: 0.02)            RandomSS  0.02  25   \n",
      "4        DivImageMSum (tol: 0.04)        DivImageMSum  0.04  25   \n",
      "5        DivImageMMin (tol: 0.04)        DivImageMMin  0.04  25   \n",
      "6            RandomSS (tol: 0.04)            RandomSS  0.04  25   \n",
      "7                             PBM                 PBM   NaN  25   \n",
      "8              PBM_Intersectional  PBM_Intersectional   NaN  25   \n",
      "9                      DebiasClip          DebiasClip   NaN  25   \n",
      "10  TrueConceptLabels (tol: 0.02)   TrueConceptLabels  0.02  25   \n",
      "11  TrueConceptLabels (tol: 0.04)   TrueConceptLabels  0.04  25   \n",
      "\n",
      "    Avg_Precision  Avg_Recall  Avg_PutI  Avg_AbsBias_gender  Avg_Bias_gender  \\\n",
      "0        0.562667    0.207950  0.562667            0.717333        -0.328000   \n",
      "1        0.470222    0.172200  0.470222            0.445333        -0.315556   \n",
      "2        0.481778    0.180772  0.481778            0.516444        -0.354667   \n",
      "3        0.490667    0.176623  0.490667            0.696000        -0.328000   \n",
      "4        0.308444    0.107551  0.308444            0.164444        -0.130667   \n",
      "5        0.352889    0.129073  0.352889            0.297778        -0.239111   \n",
      "6        0.290667    0.101371  0.290667            0.591111        -0.326222   \n",
      "7        0.503111    0.186308  0.503111            0.255111        -0.128889   \n",
      "8        0.438222    0.157611  0.438222            0.409778         0.144889   \n",
      "9        0.639111    0.235748  0.639111            0.720889        -0.265778   \n",
      "10       0.545778    0.202590  0.545778            0.432889        -0.320889   \n",
      "11       0.505778    0.186281  0.505778            0.123556        -0.096889   \n",
      "\n",
      "    Avg_Skew_gender  Avg_Abs_Skew_gender  Avg_AbsBias_for_Accurate_gender  \\\n",
      "0         -0.297615             0.575685                         0.774229   \n",
      "1          0.081650             0.513931                         0.581956   \n",
      "2         -0.016771             0.541970                         0.645362   \n",
      "3         -0.252520             0.550601                         0.724928   \n",
      "4          0.628485             0.898843                         0.538332   \n",
      "5          0.449795             0.730600                         0.578332   \n",
      "6         -0.040339             0.549889                         0.708067   \n",
      "7          0.604562             0.878847                         0.525398   \n",
      "8          0.868554             0.907814                         0.588982   \n",
      "9         -0.230910             0.416600                         0.785894   \n",
      "10         0.086705             0.531442                         0.642677   \n",
      "11         0.673294             0.946190                         0.494145   \n",
      "\n",
      "    Avg_Max_MC_Bias  \n",
      "0          0.717333  \n",
      "1          0.445333  \n",
      "2          0.516444  \n",
      "3          0.696000  \n",
      "4          0.164444  \n",
      "5          0.297778  \n",
      "6          0.591111  \n",
      "7          0.255111  \n",
      "8          0.409778  \n",
      "9          0.720889  \n",
      "10         0.432889  \n",
      "11         0.123556  \n"
     ]
    }
   ],
   "source": [
    "def run_analysis(method_call, k, tol, result_dict, catagories, catagorical_column, label_indistinguishable_values_list, protected_columns, protected_positive_values, protected_negative_values, true_rates, totals_by_cat, prefix=\"This is a picture of a\"):\n",
    "    result_dict['k'] = k\n",
    "    result_dict['tol'] = tol\n",
    "    result_dict['precision'] = np.zeros(len(catagories))\n",
    "    result_dict['precision_up_to_indistinguishablity'] = np.zeros(len(catagories))\n",
    "    result_dict['recall'] = np.zeros(len(catagories))\n",
    "    result_dict['bias'] = np.zeros((len(catagories), len(protected_columns)))\n",
    "    result_dict['abs_bias'] = np.zeros((len(catagories), len(protected_columns)))\n",
    "    result_dict['skew'] = np.zeros((len(catagories), len(protected_columns)))\n",
    "    result_dict['bias_for_accurate'] = np.zeros((len(catagories), len(protected_columns)))\n",
    "    result_dict['abs_bias_for_accurate'] = np.zeros((len(catagories), len(protected_columns)))\n",
    "    result_dict['skews_for_accurate'] = np.zeros((len(catagories), len(protected_columns)))\n",
    "    result_dict['max_skew'] = np.zeros(len(catagories))\n",
    "    result_dict['min_skew'] = np.zeros(len(catagories))\n",
    "    result_dict['worst_multiclass_error'] = np.zeros(len(catagories))\n",
    "\n",
    "\n",
    "\n",
    "    for i, cat in enumerate(catagories):\n",
    "        res = method_call(f\"{prefix} {cat}\")\n",
    "        result_dict['precision'][i] = precision(label_column=catagorical_column, positive_label_value=cat, data=res)\n",
    "        result_dict['precision_up_to_indistinguishablity'][i] = precision_up_to_indistinguishablity(label_column=catagorical_column, indistinguishable_labels=label_indistinguishable_values_list[i], data=res)\n",
    "        result_dict['recall'][i] = recall(label_column=catagorical_column, positive_label_value=cat, data=res, total_positive=totals_by_cat[cat])\n",
    "        biases = np.zeros(len(protected_columns))\n",
    "        skews = np.zeros(len(protected_columns))\n",
    "        abs_biases = np.zeros(len(protected_columns))\n",
    "        biases_for_accurate = np.zeros(len(protected_columns))\n",
    "        abs_biases_for_accurate = np.zeros(len(protected_columns))\n",
    "        skews_for_accurate = np.zeros(len(protected_columns))\n",
    "\n",
    "        for j, (protected_col, protected_pos_vals, protected_neg_vals) in enumerate(zip(protected_columns, protected_positive_values, protected_negative_values)):\n",
    "            abs_bias, abs_bias_in_correct_retrievals = abs_bias_in_retrieval(label_column=catagorical_column, positive_label_values=[cat], data=res, protected_column=protected_col, protected_positive_values=protected_pos_vals, protected_negative_values=protected_neg_vals)\n",
    "            bias, bias_in_correct_retrievals = bias_in_retrieval(label_column=catagorical_column, positive_label_values=[cat], data=res, protected_column=protected_col, protected_positive_values=protected_pos_vals, protected_negative_values=protected_neg_vals)\n",
    "            biases[j] = bias\n",
    "            biases_for_accurate[j] = bias_in_correct_retrievals\n",
    "            abs_biases[j] = abs_bias\n",
    "            abs_biases_for_accurate[j] = abs_bias_in_correct_retrievals\n",
    "            skew, skew_for_accurate = skew_in_retrieval(label_column=catagorical_column, positive_label_values=[cat], data=res, protected_column=protected_col, protected_positive_values=protected_pos_vals, true_rate=true_rates[j][i])\n",
    "            skews[j] = skew\n",
    "            skews_for_accurate[j] = skew_for_accurate\n",
    "        \n",
    "        result_dict['bias'][i] = biases\n",
    "        result_dict['abs_bias'][i] = abs_biases\n",
    "        result_dict['skew'][i] = skews\n",
    "        result_dict['bias_for_accurate'][i] = biases_for_accurate\n",
    "        result_dict['abs_bias_for_accurate'][i] = abs_biases_for_accurate\n",
    "        result_dict['skews_for_accurate'][i] = skews_for_accurate\n",
    "        result_dict['max_skew'][i] = max(skews)\n",
    "        result_dict['min_skew'][i] = min(skews)\n",
    "        result_dict['worst_multiclass_error'][i] = multiclass_bias_in_retrieval(label_column=catagorical_column, positive_label_values=[cat], data=res, protected_columns=protected_columns, protected_positive_values=protected_positive_values, protected_negative_values=protected_negative_values)\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def parse_analysis(result_dicts, protected_column_names):\n",
    "    data = []\n",
    "    for result in result_dicts:\n",
    "        parsed_result = {}\n",
    "        if result['tol'] is not None:\n",
    "            if result['name'][0:3] == \"PBM\":\n",
    "                parsed_result['method'] = result['name'] + \"(eps \" + str(result['tol']) + \")\"\n",
    "            parsed_result['method'] = result['name'] + \" (tol: \" + str(result['tol']) + \")\"\n",
    "        else:\n",
    "            parsed_result['method'] = result['name']\n",
    "        parsed_result['name'] = result['name']\n",
    "        parsed_result['tol'] = result['tol']\n",
    "        parsed_result['k'] = result['k']\n",
    "        parsed_result['Avg_Precision'] = np.mean(result['precision'])\n",
    "        parsed_result['Avg_Recall'] = np.mean(result['recall'])\n",
    "        parsed_result['Avg_PutI'] = np.mean(result['precision_up_to_indistinguishablity'])\n",
    "\n",
    "        for i, protected_col in enumerate(protected_column_names):\n",
    "            parsed_result[f'Avg_AbsBias_{protected_col}'] = np.mean(result['abs_bias'][:,i])\n",
    "            parsed_result[f'Avg_Bias_{protected_col}'] = np.mean(result['bias'][:,i])\n",
    "            parsed_result[f'Avg_Skew_{protected_col}'] = np.mean(result['skew'][:,i])\n",
    "            parsed_result[f'Avg_Abs_Skew_{protected_col}'] = np.mean(np.abs(result['skew'][:,i]))\n",
    "            parsed_result[f'Avg_AbsBias_for_Accurate_{protected_col}'] = np.mean(result['abs_bias_for_accurate'][:,i])\n",
    "\n",
    "        parsed_result['Avg_Max_MC_Bias'] =np.mean(result['worst_multiclass_error'])\n",
    "\n",
    "        if len(protected_column_names) > 1:\n",
    "            parsed_result['Max_AbsBias'] = np.max([np.mean(result['abs_bias'][:,i]) for i in range(len(protected_column_names))])\n",
    "\n",
    "        data.append(parsed_result.copy())\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "image_database.sensitive_attributes([(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")]) #\n",
    "image_database.define_pbm_classes([\"unknown gender\", \"man\", \"woman\"])\n",
    "debias_database = ImageDatabase(features_debias, data, model_debias, preprocess_debias, device_d)\n",
    "\n",
    "baseline_metrics = {'name': 'Baseline'}\n",
    "random_from_similar_set_metrics = {'name': 'RandomSS'}\n",
    "divimage_max_sum_metrics = {'name': 'DivImageMSum'}\n",
    "divimage_max_min_metrics = {'name': 'DivImageMMin'}\n",
    "#diviamge_clustering_metrics = {'name': 'DivImageC'}\n",
    "#divimage_qp_metrics = {'name': 'DivImageQP'}\n",
    "random_from_similar_set_metrics_larger_tol = {'name': 'RandomSS'}\n",
    "divimage_max_sum_metrics_larger_tol = {'name': 'DivImageMSum'}\n",
    "divimage_max_min_metrics_larger_tol = {'name': 'DivImageMMin'}\n",
    "pbm_metrics = {'name': 'PBM'}\n",
    "pbm_intersectional_metrics = {'name': 'PBM_Intersectional'}\n",
    "debias_clip_metrics = {'name': 'DebiasClip'}\n",
    "true_labels = {'name': 'TrueConceptLabels'}\n",
    "true_labels_larger_tol = {'name': 'TrueConceptLabels'}\n",
    "\n",
    "\n",
    "indistinguisable_values = [[cat] for cat in catagories]\n",
    "true_rates = [[data[data['search_term'] == cat].iloc[0].search_p_women for cat in catagories]]\n",
    "totals_by_cat = {cat: len(data[data['search_term'] == cat]) for cat in catagories}\n",
    "image_database.define_coordinate_mapping(['image_gender'], [['man']], [['woman']])\n",
    "\n",
    "run_analysis(lambda x: image_database.search(x, 25), 25, None, baseline_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 25, tol=.02, method='max_sum'), 25, .02, divimage_max_sum_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 25, tol=.02, method='max_min'), 25, .02, divimage_max_min_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 25, tol=.02, method='random'), 25, .02, random_from_similar_set_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 25, tol=.04, method='max_sum'), 25, .04, divimage_max_sum_metrics_larger_tol, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 25, tol=.04, method='max_min'), 25, .04, divimage_max_min_metrics_larger_tol, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 25, tol=.04, method='random'), 25, .04, random_from_similar_set_metrics_larger_tol, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.pbm(x, 25, eps=0), 25, None, pbm_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "image_database.define_pbm_classes([\"unknown gender and skin-tone\", \"light-skinned man\", \"light-skinned woman\", \"dark-skinned man\", \"dark-skinned woman\"])\n",
    "run_analysis(lambda x: image_database.pbm(x, 25, eps=0), 25, None, pbm_intersectional_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: debias_database.search(x, 25), 25, None, debias_clip_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 25, tol=.02, method='true_labels'), 25, .02, true_labels, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "run_analysis(lambda x: image_database.distinct_retrival(x, 25, tol=.04, method='true_labels'), 25, .04, true_labels_larger_tol, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "\n",
    "\n",
    "df = parse_analysis([baseline_metrics, divimage_max_sum_metrics, divimage_max_min_metrics, random_from_similar_set_metrics, divimage_max_sum_metrics_larger_tol, divimage_max_min_metrics_larger_tol, random_from_similar_set_metrics_larger_tol, pbm_metrics, pbm_intersectional_metrics, debias_clip_metrics, true_labels, true_labels_larger_tol], ['gender'])\n",
    "\"\"\"plt.hist(baseline_metrics['precision'] - divimage_metrics['precision'])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(baseline_metrics['bias'] - divimage_metrics['bias'])\n",
    "plt.show()\"\"\"\n",
    "\n",
    "#print(f\"Baseline Average Bias: {np.mean(baseline_metrics['bias'])}, DivImage Average Bias: {np.mean(divimage_metrics['bias'])}\")\n",
    "\n",
    "#print(divimage_metrics['bias'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       woman\n",
      "1         man\n",
      "2         man\n",
      "3         man\n",
      "4         man\n",
      "        ...  \n",
      "3257      man\n",
      "3258      man\n",
      "3259    woman\n",
      "3260    woman\n",
      "3261      man\n",
      "Length: 3262, dtype: object\n",
      "[0.00000000e+00 3.87142000e-03 1.91317163e-02 1.64907882e-02\n",
      " 9.49270030e-03 1.12148758e-02 4.47312266e-03 2.03583380e-02\n",
      " 1.10346368e-02 1.86667648e-04 8.16626831e-04 3.62853726e-03\n",
      " 3.98188250e-03 2.23206765e-02 4.24616406e-03 0.00000000e+00\n",
      " 7.52978025e-02 3.21726285e-03 6.98949508e-02 4.85670179e-03\n",
      " 9.03203516e-03 0.00000000e+00 0.00000000e+00 3.53772322e-03\n",
      " 1.29898035e-02 7.49105536e-03 2.07836451e-02 3.21196420e-02\n",
      " 7.64638872e-03 5.34165362e-03 1.94762717e-02 7.28252873e-03\n",
      " 1.74197222e-02 5.46450171e-02 0.00000000e+00 0.00000000e+00\n",
      " 5.24456374e-03 4.22269281e-02 5.30703777e-03 3.97966682e-02\n",
      " 5.77575301e-04 1.17727730e-03 1.75720372e-03 1.02663298e-02\n",
      " 1.18523965e-02 4.03718898e-03 3.09729804e-03 8.99047866e-03\n",
      " 3.64764194e-02 1.55019755e-02 3.34913484e-02 0.00000000e+00\n",
      " 4.76376569e-03 0.00000000e+00 5.67275718e-03 0.00000000e+00\n",
      " 2.41410795e-02 0.00000000e+00 2.55450076e-03 3.10233463e-02\n",
      " 0.00000000e+00 1.99245159e-02 6.42454727e-04 3.19209290e-02\n",
      " 1.00145101e-02 8.20294113e-03 2.75916580e-02 1.21197906e-01\n",
      " 7.21627367e-03 2.55859123e-02 8.66033656e-03 9.66048861e-03\n",
      " 2.56757174e-02 8.47455553e-03 1.51729745e-02 3.70017957e-03\n",
      " 1.54956846e-03 1.52610238e-02 5.68020687e-03 1.57821641e-02\n",
      " 6.87068892e-03 2.13007441e-02 1.03483150e-03 6.42188426e-02\n",
      " 1.53247468e-03 1.01936895e-02 3.79417964e-02 4.25820155e-03\n",
      " 9.35238067e-03 3.79400588e-03 0.00000000e+00 3.54591790e-03\n",
      " 1.27934093e-02 1.30231295e-02 2.41702608e-02 2.46213064e-02\n",
      " 1.43594564e-02 6.39030225e-03 3.35754776e-02 1.22801044e-02\n",
      " 1.94638230e-03 7.85910881e-03 3.41012581e-02 5.41597044e-03\n",
      " 1.17452106e-02 0.00000000e+00 9.58269589e-02 8.89331027e-03\n",
      " 1.77828896e-02 8.70312965e-03 1.59377883e-02 3.31392234e-02\n",
      " 0.00000000e+00 0.00000000e+00 3.75079415e-04 8.06482040e-03\n",
      " 0.00000000e+00 7.75601784e-04 1.62959565e-02 9.25735995e-03\n",
      " 1.46050943e-02 5.27849262e-02 9.87858641e-03 3.18210883e-03\n",
      " 1.77440956e-03 7.98739980e-03 0.00000000e+00 2.36691129e-02\n",
      " 1.57898021e-02 0.00000000e+00 1.66878708e-02 0.00000000e+00\n",
      " 1.74212277e-02 1.63937534e-02 9.24435816e-03 6.54955113e-03\n",
      " 9.88258002e-03 4.28240153e-04 1.77605973e-03 1.17106701e-02\n",
      " 4.99861802e-03 0.00000000e+00 1.83296095e-02 1.26753107e-02\n",
      " 1.73171204e-04 0.00000000e+00 1.84593041e-02 0.00000000e+00\n",
      " 6.09165610e-03 9.01456386e-02 3.08255387e-03 1.51741092e-02\n",
      " 5.99365322e-03 2.22359079e-02 1.15778549e-03 0.00000000e+00\n",
      " 5.73058060e-03 2.18242260e-02 0.00000000e+00 9.10707849e-03\n",
      " 3.52987875e-03 3.47737165e-03 4.57500266e-03 2.19554392e-02\n",
      " 6.52006709e-03 1.58284539e-02 2.01064104e-02 1.20014979e-02\n",
      " 1.18262284e-02 1.88482851e-02 3.07597427e-03 2.39078732e-02\n",
      " 2.13175692e-03 1.83442616e-02 9.33256730e-03 1.35613435e-02\n",
      " 1.28498896e-02 1.25097221e-02 1.08513753e-01 3.80813651e-03\n",
      " 2.05794801e-02 4.51366600e-02 0.00000000e+00 1.01124647e-02\n",
      " 1.06548462e-02 8.72583645e-03 1.03123944e-03 0.00000000e+00\n",
      " 4.27885431e-02 8.21513838e-04 2.77332065e-02 2.71451789e-02\n",
      " 3.15729113e-02 1.96338507e-03 1.12374607e-02 1.43818400e-02\n",
      " 2.41749057e-02 7.74977617e-03 2.38313052e-02 1.12437618e-02\n",
      " 0.00000000e+00 6.30214499e-03 3.02116942e-03 8.97368267e-03\n",
      " 2.63143597e-03 8.14290894e-03 2.42775475e-02 1.30038918e-02\n",
      " 5.65989816e-03 5.13435905e-03 1.62920282e-01 2.17334855e-02\n",
      " 0.00000000e+00 7.51399920e-03 6.76583403e-03 4.92628837e-04\n",
      " 0.00000000e+00 3.34903714e-02 9.29611034e-02 1.50518894e-03\n",
      " 1.83704142e-02 1.26193846e-02 8.17804928e-03 7.82335635e-03\n",
      " 1.22752447e-02 8.96499231e-03 2.34150861e-02 3.59812274e-03\n",
      " 0.00000000e+00 8.34250152e-03 6.01789372e-03 7.21586950e-03\n",
      " 1.80724706e-02 1.32526584e-02 3.42742058e-02 3.89390414e-02\n",
      " 5.22791952e-03 0.00000000e+00 1.74096271e-02 9.19158136e-03\n",
      " 1.61825549e-02 1.14706542e-05 2.53848546e-02 4.52469393e-03\n",
      " 5.03352110e-02 1.04019408e-02 1.57831118e-02 2.60790814e-02\n",
      " 4.15652231e-03 3.03880413e-02 2.08577449e-02 9.66856590e-03\n",
      " 1.40312915e-02 0.00000000e+00 1.17212319e-02 3.76567093e-03\n",
      " 1.22130664e-03 4.34317741e-03 1.52357113e-02 0.00000000e+00\n",
      " 1.72240570e-02 1.02817222e-02 0.00000000e+00 7.34193730e-03\n",
      " 7.82397292e-03 2.70564949e-02 1.71616612e-02 5.84695362e-03\n",
      " 1.39119337e-03 4.30257417e-03 0.00000000e+00 4.42974136e-02\n",
      " 1.48734759e-02 4.98970139e-04 2.00003004e-02 1.72965353e-02\n",
      " 1.22153635e-02 9.48419714e-03 4.76541300e-03 7.29758805e-03\n",
      " 2.88850337e-02 5.52887200e-03 9.49165077e-03 1.03110353e-02\n",
      " 0.00000000e+00 3.88715253e-03 9.62659965e-02 2.97177481e-02\n",
      " 4.71076983e-03 8.89290234e-03 1.48199330e-02 3.09681231e-03\n",
      " 1.31272335e-02 0.00000000e+00 3.00996921e-02 1.92787601e-02\n",
      " 4.51944285e-03 1.27091412e-02 1.45740622e-02 3.85683343e-02\n",
      " 4.09548603e-02 1.35068270e-02 1.53317989e-03 8.65666873e-03\n",
      " 2.07031265e-03 2.59984640e-01 0.00000000e+00 4.16228539e-03\n",
      " 7.23127388e-03 1.12130626e-02 4.24048835e-03 1.12213281e-02\n",
      " 1.25829559e-02 1.89423169e-02 1.21212489e-02 0.00000000e+00\n",
      " 3.58178600e-02 2.12295032e-02 2.67064874e-02 6.23716479e-02\n",
      " 3.91982593e-03 2.92305840e-02 6.83620763e-03 1.41742896e-02\n",
      " 1.12319769e-02 1.25368763e-02 0.00000000e+00 0.00000000e+00\n",
      " 1.97386867e-02 2.94479004e-02 2.78271283e-02 5.07708435e-03\n",
      " 1.29838886e-02 5.16982602e-03 4.87898313e-03 1.68728104e-02\n",
      " 0.00000000e+00 7.03026565e-03 0.00000000e+00 4.46222155e-03\n",
      " 8.06991655e-03 2.35533089e-02 4.89776920e-03 2.89967905e-02\n",
      " 4.65842195e-03 1.03356414e-02 8.59898606e-03 8.72144523e-03\n",
      " 6.58883744e-03 3.58184576e-03 0.00000000e+00 5.05547760e-03\n",
      " 4.84294433e-03 4.15629353e-04 1.54786375e-03 6.04409922e-03\n",
      " 6.48831844e-03 8.71306735e-02 9.75881931e-03 0.00000000e+00\n",
      " 5.12658050e-02 1.25182724e-02 2.28300639e-04 0.00000000e+00\n",
      " 1.57444664e-01 0.00000000e+00 1.25027614e-02 1.40004432e-01\n",
      " 7.33740902e-03 1.72647945e-02 1.57316168e-02 4.76831655e-03\n",
      " 6.58711307e-04 0.00000000e+00 1.50841699e-03 1.02151295e-02\n",
      " 1.24894002e-01 1.72492475e-02 5.00950643e-03 7.40165054e-03\n",
      " 2.28488585e-02 1.76818805e-02 1.84046572e-02 9.55006800e-03\n",
      " 0.00000000e+00 4.03938270e-03 0.00000000e+00 2.32482650e-02\n",
      " 7.02954191e-03 4.99246080e-02 4.01685303e-02 0.00000000e+00\n",
      " 0.00000000e+00 3.53387792e-02 0.00000000e+00 1.96847129e-02\n",
      " 2.00697904e-02 3.20221959e-02 4.76347035e-03 3.04875410e-03\n",
      " 4.93954473e-03 1.77031103e-02 7.44652922e-03 1.07686269e-02\n",
      " 1.70384185e-02 1.67215963e-02 2.63700506e-02 1.52854395e-02\n",
      " 2.04944095e-03 6.37184664e-03 1.42146162e-02 7.36779277e-02\n",
      " 2.02110791e-04 1.10347920e-02 2.61248311e-02 7.84425938e-03\n",
      " 0.00000000e+00 0.00000000e+00 2.68026959e-02 4.91894865e-03\n",
      " 4.60112801e-04 2.67148595e-02 1.16754687e-02 5.96186686e-02\n",
      " 0.00000000e+00 5.20412725e-03 2.70507573e-02 2.80095844e-03\n",
      " 5.29820332e-02 2.34521021e-02 8.95201658e-03 6.76717168e-03\n",
      " 1.18881820e-01 1.38818153e-02 6.46749255e-03 5.36189538e-03\n",
      " 4.00869113e-03 3.61773172e-03 1.55416482e-02 6.98047451e-03\n",
      " 1.91191453e-02 6.07777552e-02 9.51987895e-03 1.23097786e-03\n",
      " 2.45878621e-03 1.12314337e-02 2.43690401e-02 5.63407537e-03\n",
      " 1.90271099e-02 9.30423109e-04 6.43429030e-02 1.15399143e-03\n",
      " 1.46085394e-03 4.51669452e-03 3.17892839e-03 0.00000000e+00\n",
      " 7.10182904e-03 1.80537472e-02 1.06600988e-02 5.63802897e-03\n",
      " 1.39637014e-02 1.12245341e-02 2.32506715e-03 1.27556902e-02\n",
      " 1.82521822e-03 1.47334219e-02 9.87963773e-03 0.00000000e+00\n",
      " 8.54070870e-03 8.96125169e-03 0.00000000e+00 2.75985655e-02\n",
      " 9.19092354e-03 1.81431905e-03 2.40867378e-03 6.54621980e-03\n",
      " 1.67158087e-03 9.13752928e-04 2.19286267e-02 1.29233906e-02\n",
      " 1.72910065e-02 6.69551535e-02 2.10745332e-02 9.64979602e-04\n",
      " 2.54143485e-02 1.91628684e-02 0.00000000e+00 2.23225693e-02\n",
      " 1.85709365e-02 0.00000000e+00 1.38520152e-02 2.62628105e-02\n",
      " 1.16739051e-02 5.19863954e-02 1.77258523e-03 1.02214847e-02\n",
      " 5.92452291e-03 0.00000000e+00 2.48711392e-02 0.00000000e+00\n",
      " 2.50174188e-04 1.52791687e-02 4.39931163e-03 5.86785945e-03\n",
      " 6.44509679e-03 9.08858761e-03 6.46889035e-03 1.69574370e-03\n",
      " 0.00000000e+00 1.31581073e-02 3.77699386e-04 4.76525734e-03]\n",
      "[2.59984640e-01 1.62920282e-01 1.57444664e-01 1.40004432e-01\n",
      " 1.24894002e-01 1.21197906e-01 1.18881820e-01 1.08513753e-01\n",
      " 9.62659965e-02 9.58269589e-02 9.29611034e-02 9.01456386e-02\n",
      " 8.71306735e-02 7.52978025e-02 7.36779277e-02 6.98949508e-02\n",
      " 6.69551535e-02 6.43429030e-02 6.42188426e-02 6.23716479e-02\n",
      " 6.07777552e-02 5.96186686e-02 5.46450171e-02 5.29820332e-02\n",
      " 5.27849262e-02 5.19863954e-02 5.12658050e-02 5.03352110e-02\n",
      " 4.99246080e-02 4.51366600e-02 4.42974136e-02 4.27885431e-02\n",
      " 4.22269281e-02 4.09548603e-02 4.01685303e-02 3.97966682e-02\n",
      " 3.89390414e-02 3.85683343e-02 3.79417964e-02 3.64764194e-02\n",
      " 3.58178600e-02 3.53387792e-02 3.42742058e-02 3.41012581e-02\n",
      " 3.35754776e-02 3.34913484e-02 3.34903714e-02 3.31392234e-02\n",
      " 3.21196420e-02 3.20221959e-02 3.19209290e-02 3.15729113e-02\n",
      " 3.10233463e-02 3.03880413e-02 3.00996921e-02 2.97177481e-02\n",
      " 2.94479004e-02 2.92305840e-02 2.89967905e-02 2.88850337e-02\n",
      " 2.78271283e-02 2.77332065e-02 2.75985655e-02 2.75916580e-02\n",
      " 2.71451789e-02 2.70564949e-02 2.70507573e-02 2.68026959e-02\n",
      " 2.67148595e-02 2.67064874e-02 2.63700506e-02 2.62628105e-02\n",
      " 2.61248311e-02 2.60790814e-02 2.56757174e-02 2.55859123e-02\n",
      " 2.54143485e-02 2.53848546e-02 2.48711392e-02 2.46213064e-02\n",
      " 2.43690401e-02 2.42775475e-02 2.41749057e-02 2.41702608e-02\n",
      " 2.41410795e-02 2.39078732e-02 2.38313052e-02 2.36691129e-02\n",
      " 2.35533089e-02 2.34521021e-02 2.34150861e-02 2.32482650e-02\n",
      " 2.28488585e-02 2.23225693e-02 2.23206765e-02 2.22359079e-02\n",
      " 2.19554392e-02 2.19286267e-02 2.18242260e-02 2.17334855e-02\n",
      " 2.13007441e-02 2.12295032e-02 2.10745332e-02 2.08577449e-02\n",
      " 2.07836451e-02 2.05794801e-02 2.03583380e-02 2.01064104e-02\n",
      " 2.00697904e-02 2.00003004e-02 1.99245159e-02 1.97386867e-02\n",
      " 1.96847129e-02 1.94762717e-02 1.92787601e-02 1.91628684e-02\n",
      " 1.91317163e-02 1.91191453e-02 1.90271099e-02 1.89423169e-02\n",
      " 1.88482851e-02 1.85709365e-02 1.84593041e-02 1.84046572e-02\n",
      " 1.83704142e-02 1.83442616e-02 1.83296095e-02 1.80724706e-02\n",
      " 1.80537472e-02 1.77828896e-02 1.77031103e-02 1.76818805e-02\n",
      " 1.74212277e-02 1.74197222e-02 1.74096271e-02 1.72965353e-02\n",
      " 1.72910065e-02 1.72647945e-02 1.72492475e-02 1.72240570e-02\n",
      " 1.71616612e-02 1.70384185e-02 1.68728104e-02 1.67215963e-02\n",
      " 1.66878708e-02 1.64907882e-02 1.63937534e-02 1.62959565e-02\n",
      " 1.61825549e-02 1.59377883e-02 1.58284539e-02 1.57898021e-02\n",
      " 1.57831118e-02 1.57821641e-02 1.57316168e-02 1.55416482e-02\n",
      " 1.55019755e-02 1.52854395e-02 1.52791687e-02 1.52610238e-02\n",
      " 1.52357113e-02 1.51741092e-02 1.51729745e-02 1.48734759e-02\n",
      " 1.48199330e-02 1.47334219e-02 1.46050943e-02 1.45740622e-02\n",
      " 1.43818400e-02 1.43594564e-02 1.42146162e-02 1.41742896e-02\n",
      " 1.40312915e-02 1.39637014e-02 1.38818153e-02 1.38520152e-02\n",
      " 1.35613435e-02 1.35068270e-02 1.32526584e-02 1.31581073e-02\n",
      " 1.31272335e-02 1.30231295e-02 1.30038918e-02 1.29898035e-02\n",
      " 1.29838886e-02 1.29233906e-02 1.28498896e-02 1.27934093e-02\n",
      " 1.27556902e-02 1.27091412e-02 1.26753107e-02 1.26193846e-02\n",
      " 1.25829559e-02 1.25368763e-02 1.25182724e-02 1.25097221e-02\n",
      " 1.25027614e-02 1.22801044e-02 1.22752447e-02 1.22153635e-02\n",
      " 1.21212489e-02 1.20014979e-02 1.18523965e-02 1.18262284e-02\n",
      " 1.17452106e-02 1.17212319e-02 1.17106701e-02 1.16754687e-02\n",
      " 1.16739051e-02 1.12437618e-02 1.12374607e-02 1.12319769e-02\n",
      " 1.12314337e-02 1.12245341e-02 1.12213281e-02 1.12148758e-02\n",
      " 1.12130626e-02 1.10347920e-02 1.10346368e-02 1.07686269e-02\n",
      " 1.06600988e-02 1.06548462e-02 1.04019408e-02 1.03356414e-02\n",
      " 1.03110353e-02 1.02817222e-02 1.02663298e-02 1.02214847e-02\n",
      " 1.02151295e-02 1.01936895e-02 1.01124647e-02 1.00145101e-02\n",
      " 9.88258002e-03 9.87963773e-03 9.87858641e-03 9.75881931e-03\n",
      " 9.66856590e-03 9.66048861e-03 9.55006800e-03 9.51987895e-03\n",
      " 9.49270030e-03 9.49165077e-03 9.48419714e-03 9.35238067e-03\n",
      " 9.33256730e-03 9.25735995e-03 9.24435816e-03 9.19158136e-03\n",
      " 9.19092354e-03 9.10707849e-03 9.08858761e-03 9.03203516e-03\n",
      " 8.99047866e-03 8.97368267e-03 8.96499231e-03 8.96125169e-03\n",
      " 8.95201658e-03 8.89331027e-03 8.89290234e-03 8.72583645e-03\n",
      " 8.72144523e-03 8.70312965e-03 8.66033656e-03 8.65666873e-03\n",
      " 8.59898606e-03 8.54070870e-03 8.47455553e-03 8.34250152e-03\n",
      " 8.20294113e-03 8.17804928e-03 8.14290894e-03 8.06991655e-03\n",
      " 8.06482040e-03 7.98739980e-03 7.85910881e-03 7.84425938e-03\n",
      " 7.82397292e-03 7.82335635e-03 7.74977617e-03 7.64638872e-03\n",
      " 7.51399920e-03 7.49105536e-03 7.44652922e-03 7.40165054e-03\n",
      " 7.34193730e-03 7.33740902e-03 7.29758805e-03 7.28252873e-03\n",
      " 7.23127388e-03 7.21627367e-03 7.21586950e-03 7.10182904e-03\n",
      " 7.03026565e-03 7.02954191e-03 6.98047451e-03 6.87068892e-03\n",
      " 6.83620763e-03 6.76717168e-03 6.76583403e-03 6.58883744e-03\n",
      " 6.54955113e-03 6.54621980e-03 6.52006709e-03 6.48831844e-03\n",
      " 6.46889035e-03 6.46749255e-03 6.44509679e-03 6.39030225e-03\n",
      " 6.37184664e-03 6.30214499e-03 6.09165610e-03 6.04409922e-03\n",
      " 6.01789372e-03 5.99365322e-03 5.92452291e-03 5.86785945e-03\n",
      " 5.84695362e-03 5.73058060e-03 5.68020687e-03 5.67275718e-03\n",
      " 5.65989816e-03 5.63802897e-03 5.63407537e-03 5.52887200e-03\n",
      " 5.41597044e-03 5.36189538e-03 5.34165362e-03 5.30703777e-03\n",
      " 5.24456374e-03 5.22791952e-03 5.20412725e-03 5.16982602e-03\n",
      " 5.13435905e-03 5.07708435e-03 5.05547760e-03 5.00950643e-03\n",
      " 4.99861802e-03 4.93954473e-03 4.91894865e-03 4.89776920e-03\n",
      " 4.87898313e-03 4.85670179e-03 4.84294433e-03 4.76831655e-03\n",
      " 4.76541300e-03 4.76525734e-03 4.76376569e-03 4.76347035e-03\n",
      " 4.71076983e-03 4.65842195e-03 4.57500266e-03 4.52469393e-03\n",
      " 4.51944285e-03 4.51669452e-03 4.47312266e-03 4.46222155e-03\n",
      " 4.39931163e-03 4.34317741e-03 4.30257417e-03 4.25820155e-03\n",
      " 4.24616406e-03 4.24048835e-03 4.16228539e-03 4.15652231e-03\n",
      " 4.03938270e-03 4.03718898e-03 4.00869113e-03 3.98188250e-03\n",
      " 3.91982593e-03 3.88715253e-03 3.87142000e-03 3.80813651e-03\n",
      " 3.79400588e-03 3.76567093e-03 3.70017957e-03 3.62853726e-03\n",
      " 3.61773172e-03 3.59812274e-03 3.58184576e-03 3.54591790e-03\n",
      " 3.53772322e-03 3.52987875e-03 3.47737165e-03 3.21726285e-03\n",
      " 3.18210883e-03 3.17892839e-03 3.09729804e-03 3.09681231e-03\n",
      " 3.08255387e-03 3.07597427e-03 3.04875410e-03 3.02116942e-03\n",
      " 2.80095844e-03 2.63143597e-03 2.55450076e-03 2.45878621e-03\n",
      " 2.40867378e-03 2.32506715e-03 2.13175692e-03 2.07031265e-03\n",
      " 2.04944095e-03 1.96338507e-03 1.94638230e-03 1.82521822e-03\n",
      " 1.81431905e-03 1.77605973e-03 1.77440956e-03 1.77258523e-03\n",
      " 1.75720372e-03 1.69574370e-03 1.67158087e-03 1.54956846e-03\n",
      " 1.54786375e-03 1.53317989e-03 1.53247468e-03 1.50841699e-03\n",
      " 1.50518894e-03 1.46085394e-03 1.39119337e-03 1.23097786e-03\n",
      " 1.22130664e-03 1.17727730e-03 1.15778549e-03 1.15399143e-03\n",
      " 1.03483150e-03 1.03123944e-03 9.64979602e-04 9.30423109e-04\n",
      " 9.13752928e-04 8.21513838e-04 8.16626831e-04 7.75601784e-04\n",
      " 6.58711307e-04 6.42454727e-04 5.77575301e-04 4.98970139e-04\n",
      " 4.92628837e-04 4.60112801e-04 4.28240153e-04 4.15629353e-04\n",
      " 3.77699386e-04 3.75079415e-04 2.50174188e-04 2.28300639e-04\n",
      " 2.02110791e-04 1.86667648e-04 1.73171204e-04 1.14706542e-05\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "[305 210 364 367 376  67 432 178 286 106 218 149 357  16 411  18 481 450\n",
      "  83 319 441 423  33 428 121 493 360 244 389 181 271 188  37 300 390  39\n",
      " 235 299  86  48 316 393 234 102  98  50 217 111  27 397  63 192  59 249\n",
      " 294 287 329 321 343 280 330 190 471  66 191 265 426 418 421 318 406 491\n",
      " 414 247  72  69 484 242 498  95 446 206 196  94  56 171 198 127 341 429\n",
      " 226 387 380 487  13 153 163 478 157 211  81 317 482 250  26 180   7 166\n",
      " 396 274  61 328 395  30 295 485   2 440 448 313 169 488 146 382 220 173\n",
      " 142 232 457 108 401 381 132  32 238 275 480 369 377 260 266 404 335 405\n",
      " 130   3 133 118 240 110 165 128 246  79 370 438  49 407 501  77 258 151\n",
      "  74 272 290 465 120 298 195  96 410 323 252 460 433 490 175 301 233 509\n",
      " 292  93 207  24 332 479 176  92 463 297 143 221 312 325 361 177 366  99\n",
      " 224 276 314 167  44 168 104 254 139 422 492 199 194 324 445 461 311   5\n",
      " 309 413   8 403 458 184 245 345 283 261  43 495 375  85 183  64 136 466\n",
      " 122 358 251  71 383 442   4 282 277  88 174 119 134 239 472 159 505  20\n",
      "  47 203 225 469 430 107 289 185 347 109  70 303 346 468  73 229  65 222\n",
      " 205 340 115 125 101 415 264 223 197  28 213  25 402 379 263 368 279  31\n",
      " 308  68 231 456 337 388 439  80 322 431 214 348 135 475 164 356 506 434\n",
      " 504  97 409 201 148 355 230 152 496 503 267 156  78  54 208 459 447 281\n",
      " 103 435  29  38  36 236 425 333 209 331 351 378 140 400 419 342 334  19\n",
      " 352 371 278 511  52 398 288 344 162 243 296 453   6 339 502 257 269  87\n",
      "  14 310 307 248 385  45 436  12 320 285   1 179  89 255  75  11 437 227\n",
      " 349  91  23 160 161  17 123 454  46 291 150 170 399 202 427 204  58 444\n",
      " 474 462 172 304 408 193 100 464 473 138 124 494  42 507 476  76 354 302\n",
      "  84 374 219 452 268 443 256  41 154 451  82 186 483 449 477 189  10 117\n",
      " 372  62  40 273 215 420 137 353 510 114 500 362 412   9 144 241 338 113\n",
      "  21  22 116 112 455 470 187 200 105 212 467 158 182 228 126  15 155 129\n",
      " 131 147 145 486 489 497 499 141 508 216  35  34 384 350 327 326 359 363\n",
      " 365 315 306  60 373 293  57 284  55  53 336 386  51 391 392 394 270 262\n",
      "  90 416 259 253 417 424 237   0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For precision-bias and recall-bias graphs --\n",
    "\n",
    "number_of_tol_steps = 15\n",
    "methods = [\n",
    "    'max_sum', 'max_min', 'random', 'true_labels', 'PBM_gender', 'PBM_skintone', 'PBM_intersectional'\n",
    "]\n",
    "\n",
    "#methods = ['PBM', 'PBM_Intersectional']\n",
    "\n",
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "image_database.sensitive_attributes([(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a white person\", \"A picture of a black person\")])\n",
    "\n",
    "debias_database = ImageDatabase(features_debias, data, model_debias, preprocess_debias, device_d)\n",
    "\n",
    "totals_by_cat = {cat: len(data[data['search_term'] == cat]) for cat in catagories}\n",
    "image_database.define_coordinate_mapping(['image_gender'], [['man']], [['woman']])\n",
    "\n",
    "random_iters = 5\n",
    "\n",
    "for k in [10, 25, 50, 100]:\n",
    "    print(f\"Starting analysis for k: {k}...\")\n",
    "    precisions, biases, skews, abs_biases, abs_bias_for_accurate = {}, {}, {}, {}, {}\n",
    "\n",
    "    #max_skew = []\n",
    "    #min_skew = []\n",
    "\n",
    "    for method in methods:\n",
    "        print(f\"Method: {method}...\")\n",
    "        avg_precision = []\n",
    "        avg_bias = []\n",
    "        avg_skew = []\n",
    "        avg_abs_bias = []\n",
    "        avg_abs_bias_for_accurate = []\n",
    "        if method[0:3] == 'PBM':\n",
    "            if method == 'PBM_gender':\n",
    "                image_database.define_pbm_classes([\"unknown gender\", \"man\", \"woman\"])\n",
    "            elif method == 'PBM_skintone':\n",
    "                image_database.define_pbm_classes([\"unknown skin-tone\", \"light-skinned person\", \"dark-skinned person\"])\n",
    "            elif method == 'PBM_intersectional':\n",
    "                image_database.define_pbm_classes([\"unknown gender and skin-tone\", \"light-skinned man\", \"light-skinned woman\", \"dark-skinned man\", \"dark-skinned woman\"])\n",
    "            for eps in tqdm(reversed(range(0, 11))):\n",
    "                mean_precision = 0\n",
    "                mean_bias = 0\n",
    "                mean_skew = 0\n",
    "                mean_abs_bias = 0\n",
    "                mean_abs_average_bias_for_accurate = 0\n",
    "                for i in range(random_iters):\n",
    "                    baseline_metrics = {}\n",
    "                    run_analysis(lambda x: image_database.pbm(x, k, eps=eps/10), k, None, baseline_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates)\n",
    "                    mean_precision += np.mean(baseline_metrics['precision'])\n",
    "                    mean_bias += np.mean(baseline_metrics['bias'])\n",
    "                    mean_abs_bias += np.mean(baseline_metrics['abs_bias'])\n",
    "                    mean_skew += np.mean(baseline_metrics['skew'])\n",
    "                    mean_abs_average_bias_for_accurate += np.mean(baseline_metrics['abs_bias_for_accurate'])\n",
    "\n",
    "                avg_precision.append(mean_precision / random_iters)\n",
    "                avg_bias.append(mean_bias / random_iters)\n",
    "                avg_skew.append(mean_skew / random_iters)\n",
    "                avg_abs_bias.append(mean_abs_bias / random_iters)\n",
    "                avg_abs_bias_for_accurate.append(mean_abs_average_bias_for_accurate / random_iters)\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        else:\n",
    "            for tol in tqdm(range(0, number_of_tol_steps)):\n",
    "                baseline_metrics = {}\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                t = tol / (10 * (number_of_tol_steps-1))\n",
    "                ## This is inefficient write a faster way to vary tol and rerun the analysis\n",
    "                if method == 'random':\n",
    "                    mean_precision = 0\n",
    "                    mean_bias = 0\n",
    "                    mean_skew = 0\n",
    "                    mean_abs_bias = 0\n",
    "                    mean_abs_average_bias_for_accurate = 0\n",
    "                    for i in range(random_iters):\n",
    "                        baseline_metrics = {}\n",
    "                        run_analysis(lambda x: image_database.distinct_retrival(x, k, tol=t, method=method), k, t, baseline_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates)\n",
    "                        mean_precision += np.mean(baseline_metrics['precision'])\n",
    "                        mean_bias += np.mean(baseline_metrics['bias'])\n",
    "                        mean_skew += np.mean(baseline_metrics['skew'])\n",
    "                        mean_abs_bias += np.mean(baseline_metrics['abs_bias'])\n",
    "                        mean_abs_average_bias_for_accurate += np.mean(baseline_metrics['abs_bias_for_accurate'])\n",
    "                    avg_precision.append(mean_precision/random_iters)\n",
    "                    avg_bias.append(mean_bias/random_iters)\n",
    "                    avg_skew.append(mean_skew/random_iters)\n",
    "                    avg_abs_bias.append(mean_abs_bias/random_iters)\n",
    "                    avg_abs_bias_for_accurate.append(mean_abs_average_bias_for_accurate/random_iters)\n",
    "\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    run_analysis(lambda x: image_database.distinct_retrival(x, k, tol=t, method=method), k, t, baseline_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates)\n",
    "                    avg_precision.append(np.mean(baseline_metrics['precision']))\n",
    "                    avg_bias.append(np.mean(baseline_metrics['bias']))\n",
    "                    avg_skew.append(np.mean(baseline_metrics['skew']))\n",
    "                    avg_abs_bias.append(np.mean(baseline_metrics['abs_bias']))\n",
    "                    avg_abs_bias_for_accurate.append(np.mean(baseline_metrics['abs_bias_for_accurate']))\n",
    "                \n",
    "                #max_skew.append(np.mean(divimage_metrics['max_skew']))\n",
    "                #min_skew.append(np.mean(divimage_metrics['min_skew']))\n",
    "        precisions[method] = avg_precision\n",
    "        biases[method] = avg_bias\n",
    "        skews[method] = avg_skew\n",
    "        abs_biases[method] = avg_abs_bias\n",
    "        abs_bias_for_accurate[method] = avg_abs_bias_for_accurate\n",
    "\n",
    "    print(\"Computing Baseline...\")\n",
    "    baseline_metrics = {}\n",
    "    run_analysis(lambda x: image_database.search(x, k), k, None, baseline_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates)\n",
    "    baseline_precision = np.mean(baseline_metrics['precision'])\n",
    "    baseline_bias = np.mean(baseline_metrics['bias'])\n",
    "    baseline_skew = np.mean(baseline_metrics['skew'])\n",
    "    baseline_abs_bias = np.mean(baseline_metrics['abs_bias']) \n",
    "    baseline_abs_bias_for_accurate = np.mean(baseline_metrics['abs_bias_for_accurate'])\n",
    "\n",
    "\n",
    "    print(\"Computing Debias-Prompt...\")\n",
    "    debias_metrics = {}\n",
    "    run_analysis(lambda x: debias_database.search(x, k), k, None, debias_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates)\n",
    "    debias_precision = np.mean(debias_metrics['precision'])\n",
    "    debias_bias = np.mean(debias_metrics['bias'])\n",
    "    debias_skew = np.mean(debias_metrics['skew'])\n",
    "    debias_abs_bias = np.mean(debias_metrics['abs_bias'])\n",
    "    debias_abs_bias_for_accurate = np.mean(debias_metrics['abs_bias_for_accurate'])\n",
    "\n",
    "    for method in methods:\n",
    "        plt.plot(biases[method], precisions[method], label=method)\n",
    "    plt.scatter(baseline_bias, baseline_precision, label='Baseline', color = 'black')\n",
    "    plt.scatter(debias_bias, debias_precision, label='Debias', color = 'red')\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Average Bias\")\n",
    "    plt.ylabel(\"Average Precision\")\n",
    "    #plt.gca().invert_xaxis()\n",
    "    plt.title(f\"Precision-Bias Curve @ {k}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for method in methods:\n",
    "        plt.plot(abs_biases[method], precisions[method], label=method)\n",
    "    plt.scatter(baseline_abs_bias, baseline_precision, label='Baseline', color = 'black')\n",
    "    plt.scatter(debias_abs_bias, debias_precision, label='Debias', color = 'red')\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Average AbsBias\")\n",
    "    plt.ylabel(\"Average Precision\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title(f\"Precision-AbsBias Curve @ {k}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for method in methods:\n",
    "        plt.plot(abs_bias_for_accurate[method], precisions[method], label=method)\n",
    "    plt.scatter(baseline_abs_bias_for_accurate, baseline_precision, label='Baseline', color = 'black')\n",
    "    plt.scatter(debias_abs_bias_for_accurate, debias_precision, label='Debias', color = 'red')\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Average AbsBias for Accurate\")\n",
    "    plt.ylabel(\"Average Precision\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title(f\"Precision-AbsBias Curve for Accurate Labels @ {k}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    for method in methods:\n",
    "        plt.plot(skews[method], precisions[method], label=method)\n",
    "    plt.scatter(baseline_skew, baseline_precision, label='Baseline', color='black')\n",
    "    plt.scatter(debias_skew, debias_precision, label='Debias', color='red')\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Average Skew\")\n",
    "    plt.ylabel(\"Average Precision\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title(f\"Precision-Skew Curve @ {k}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for method in methods:\n",
    "        plt.scatter(biases[method], skews[method], label=method)\n",
    "    plt.scatter(baseline_bias, baseline_skew, label='Baseline', color='black')\n",
    "    plt.scatter(debias_bias, debias_skew, label='Debias', color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"Average Skew\")\n",
    "    plt.xlabel(\"Average Bias\")\n",
    "    plt.title(f\"Bias-Skew Scatter @ {k}\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m baseline_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     21\u001b[0m t \u001b[38;5;241m=\u001b[39m tol \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mrun_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct_retrival\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_sum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatagories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearch_term\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindistinguisable_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_gender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwoman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_rates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotals_by_cat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m avg_precision\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(baseline_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     24\u001b[0m avg_bias\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(baseline_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabs_bias\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\Alexa\\Documents\\Harvard\\Thesis Research\\Exploration Code\\image_database.py:320\u001b[0m, in \u001b[0;36mrun_analysis\u001b[1;34m(method_call, k, tol, result_dict, catagories, catagorical_column, label_indistinguishable_values_list, protected_columns, protected_positive_values, protected_negative_values, true_rates, totals_by_cat, prefix)\u001b[0m\n\u001b[0;32m    315\u001b[0m result_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworst_multiclass_error\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(catagories))\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, cat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(catagories):\n\u001b[1;32m--> 320\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mmethod_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcat\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     result_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m precision(label_column\u001b[38;5;241m=\u001b[39mcatagorical_column, positive_label_value\u001b[38;5;241m=\u001b[39mcat, data\u001b[38;5;241m=\u001b[39mres)\n\u001b[0;32m    322\u001b[0m     result_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision_up_to_indistinguishablity\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m precision_up_to_indistinguishablity(label_column\u001b[38;5;241m=\u001b[39mcatagorical_column, indistinguishable_labels\u001b[38;5;241m=\u001b[39mlabel_indistinguishable_values_list[i], data\u001b[38;5;241m=\u001b[39mres)\n",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m baseline_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     21\u001b[0m t \u001b[38;5;241m=\u001b[39m tol \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m run_analysis(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mimage_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct_retrival\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_sum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, k, t, baseline_metrics, catagories, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_term\u001b[39m\u001b[38;5;124m'\u001b[39m, indistinguisable_values, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_gender\u001b[39m\u001b[38;5;124m'\u001b[39m], [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m'\u001b[39m]], [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m'\u001b[39m]], true_rates, totals_by_cat)\n\u001b[0;32m     23\u001b[0m avg_precision\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(baseline_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     24\u001b[0m avg_bias\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(baseline_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabs_bias\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\Alexa\\Documents\\Harvard\\Thesis Research\\Exploration Code\\image_database.py:210\u001b[0m, in \u001b[0;36mImageDatabase.distinct_retrival\u001b[1;34m(self, query, k, max_sim_set, tol, method, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m similarities \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m@\u001b[39m query_features\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    208\u001b[0m best \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39margsort(descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m--> 210\u001b[0m sim_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc_sim_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sim_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m distinct_sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve_distinct(sim_set, similarities, k, mode\u001b[38;5;241m=\u001b[39mmethod)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[distinct_sort]\n",
      "File \u001b[1;32mc:\\Users\\Alexa\\Documents\\Harvard\\Thesis Research\\Exploration Code\\image_database.py:45\u001b[0m, in \u001b[0;36mImageDatabase._calc_sim_set\u001b[1;34m(self, best, similarities, k, max_sim_set, tol)\u001b[0m\n\u001b[0;32m     43\u001b[0m sim_top_k_avg \u001b[38;5;241m=\u001b[39m sim[\u001b[38;5;241m0\u001b[39m:k]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sim)):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msim\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msim_top_k_avg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     47\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(k, i), max_sim_set)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attr = [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\"), (\"A picture of a young person\", \"A picture of an old person\"), (\"A picture of an American Person\", \"A picture of a non-American person\"), (\"A picture of a disabled person\", \"A picture of a person who is not disabled\")]\n",
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "totals_by_cat = {cat: len(data[data['search_term'] == cat]) for cat in catagories}\n",
    "image_database.define_coordinate_mapping(['image_gender'], [['man']], [['woman']])\n",
    "\n",
    "\n",
    "ks = [10, 25, 50]\n",
    "number_of_tol_steps = 20\n",
    "\n",
    "for k in ks:\n",
    "    precisions, biases, recalls = {}, {}, {}\n",
    "    for i in tqdm(range(len(attr))):\n",
    "        avg_precision = []\n",
    "        avg_bias = []\n",
    "        avg_recall = []\n",
    "        image_database.sensitive_attributes(attr[:i+1])\n",
    "        for tol in range(0, number_of_tol_steps):\n",
    "            baseline_metrics = {}\n",
    "            t = tol / (200)\n",
    "            run_analysis(lambda x: image_database.distinct_retrival(x, k, tol=t, method='max_sum'), k, t, baseline_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "            avg_precision.append(np.mean(baseline_metrics['precision']))\n",
    "            avg_bias.append(np.mean(baseline_metrics['abs_bias']))\n",
    "            avg_recall.append(np.mean(baseline_metrics['recall']))\n",
    "        precisions[str(i)] = avg_precision\n",
    "        biases[str(i)] = avg_bias\n",
    "        recalls[str(i)] = avg_recall\n",
    "\n",
    "\n",
    "    baseline_metrics = {}\n",
    "    run_analysis(lambda x: image_database.search(x, k), k, None, baseline_metrics, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "    baseline_precision = np.mean(baseline_metrics['precision'])\n",
    "    baseline_bias = np.mean(baseline_metrics['abs_bias'])\n",
    "    baseline_recall = np.mean(baseline_metrics['recall'])\n",
    "\n",
    "    for i in range(len(attr)):\n",
    "        plt.plot(biases[str(i)], precisions[str(i)], label=(\"Total unmeasured cats: \" + str(i)))\n",
    "    plt.scatter(baseline_bias, baseline_precision, label='Baseline')\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Average AbsBias\")\n",
    "    plt.ylabel(\"Average Precision\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title(f\"Precision-AbsBias Curve @ {k}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for i in range(len(attr)):\n",
    "        plt.plot(biases[str(i)], recalls[str(i)], label=(\"Total unmeasured cats: \" + str(i)))\n",
    "    plt.scatter(baseline_bias, baseline_recall, label='Baseline')\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Average AbsBias\")\n",
    "    plt.ylabel(\"Average Recall\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title(f\"Recall-AbsBias Curve @ {k}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for i in range(len(attr)):\n",
    "        plt.plot(recalls[str(i)], precisions[str(i)], label=(\"Total unmeasured cats: \" + str(i)))\n",
    "    plt.scatter(baseline_bias, baseline_recall, label='Baseline')\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Average Recall\")\n",
    "    plt.ylabel(\"Average Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve @ {k}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for method: CLIP_gender...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:59<00:00, 14.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for method: CLIP_skintone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:21<00:00, 50.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for method: CLIP_intersectional...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:11<00:00, 47.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for method: CLIP_three_attributes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:19<00:00, 49.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for method: CDI_Min_skintone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [08:01<00:00, 120.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for method: CDI_EucSum_intersectional...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [06:15<00:00, 93.77s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for method: CDI_EucMin_intersectional...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [15:01<00:00, 225.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     method                       name  \\\n",
      "0                      CLIP_gender (tol: 0)                CLIP_gender   \n",
      "1                     CLIP_gender (tol: 20)                CLIP_gender   \n",
      "2                     CLIP_gender (tol: 40)                CLIP_gender   \n",
      "3                     CLIP_gender (tol: 60)                CLIP_gender   \n",
      "4                     CLIP_gender (tol: 80)                CLIP_gender   \n",
      "..                                      ...                        ...   \n",
      "571  CDI_EucMin_intersectional (tol: 0.055)  CDI_EucMin_intersectional   \n",
      "572   CDI_EucMin_intersectional (tol: 0.06)  CDI_EucMin_intersectional   \n",
      "573  CDI_EucMin_intersectional (tol: 0.065)  CDI_EucMin_intersectional   \n",
      "574   CDI_EucMin_intersectional (tol: 0.07)  CDI_EucMin_intersectional   \n",
      "575  CDI_EucMin_intersectional (tol: 0.075)  CDI_EucMin_intersectional   \n",
      "\n",
      "        tol    k  Avg_Precision  Avg_Recall  Avg_PutI  Avg_AbsBias_gender  \\\n",
      "0     0.000   10       0.655556    0.099015  0.655556            0.773333   \n",
      "1    20.000   10       0.662222    0.100904  0.662222            0.675556   \n",
      "2    40.000   10       0.653333    0.099032  0.653333            0.640000   \n",
      "3    60.000   10       0.648889    0.099075  0.648889            0.671111   \n",
      "4    80.000   10       0.637778    0.097143  0.637778            0.653333   \n",
      "..      ...  ...            ...         ...       ...                 ...   \n",
      "571   0.055  100       0.114667    0.160867  0.114667            0.239111   \n",
      "572   0.060  100       0.099778    0.140555  0.099778            0.221333   \n",
      "573   0.065  100       0.086444    0.121824  0.086444            0.214222   \n",
      "574   0.070  100       0.080667    0.112132  0.080667            0.211111   \n",
      "575   0.075  100       0.074000    0.104065  0.074000            0.204889   \n",
      "\n",
      "     Avg_Bias_gender  Avg_Skew_gender  Avg_Abs_Skew_gender  \\\n",
      "0          -0.391111        -0.085598             0.363828   \n",
      "1          -0.426667        -0.106780             0.430286   \n",
      "2          -0.364444        -0.005069             0.460094   \n",
      "3          -0.360000        -0.016133             0.427944   \n",
      "4          -0.386667        -0.021507             0.395755   \n",
      "..               ...              ...                  ...   \n",
      "571        -0.232000         0.529813             0.859849   \n",
      "572        -0.216000         0.559927             0.900309   \n",
      "573        -0.208889         0.573441             0.912775   \n",
      "574        -0.205778         0.578263             0.920039   \n",
      "575        -0.199556         0.587985             0.929761   \n",
      "\n",
      "     Avg_AbsBias_for_Accurate_gender  Avg_Max_MC_Bias  \n",
      "0                           0.783792         0.773333  \n",
      "1                           0.698907         0.675556  \n",
      "2                           0.708907         0.640000  \n",
      "3                           0.759118         0.671111  \n",
      "4                           0.725062         0.653333  \n",
      "..                               ...              ...  \n",
      "571                         0.541530         0.239111  \n",
      "572                         0.519194         0.221333  \n",
      "573                         0.550560         0.214222  \n",
      "574                         0.557331         0.211111  \n",
      "575                         0.558751         0.204889  \n",
      "\n",
      "[576 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_database = ImageDatabase(features, data, model, preprocess, device)\n",
    "totals_by_cat = {cat: len(data[data['search_term'] == cat]) for cat in catagories}\n",
    "image_database.define_coordinate_mapping(['image_gender'], [['man']], [['woman']])\n",
    "indistinguisable_values = [[cat] for cat in catagories]\n",
    "true_rates = [[data[data['search_term'] == cat].iloc[0].search_p_women for cat in catagories]]\n",
    "totals_by_cat = {cat: len(data[data['search_term'] == cat]) for cat in catagories}\n",
    "\n",
    "image_database.add_clipclip_ordering(\"gender\", np.load('datasets/MI_orders/gender.npy'))\n",
    "image_database.add_clipclip_ordering(\"skintone\", np.load('datasets/MI_orders/skintone.npy'))\n",
    "image_database.add_clipclip_ordering(\"intersectional\", np.load('datasets/MI_orders/gender_skintone.npy'))\n",
    "image_database.add_clipclip_ordering(\"three_attr\", np.load('datasets/MI_orders/intersectional.npy'))\n",
    "\n",
    "\n",
    "\"\"\"method_name_specification_list = [\n",
    "    (lambda k, tol: lambda x: image_database.search(x, k), 'Baseline', []),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_gender', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_sum'), 'CDI_Sum_skintone', [(\"sensitive_attributes\", [(\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_gender', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_skintone', [(\"sensitive_attributes\", [(\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='random'), 'CDI_Random', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='true_labels'), 'CDI_TrueConcept', []),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_gender', [(\"pbm_classes\", [\"unknown gender\", \"man\", \"woman\"])]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_intersectional', [(\"pbm_classes\", [\"unknown gender and skin-tone\", \"light-skinned man\", \"light-skinned woman\", \"dark-skinned man\", \"dark-skinned woman\"])]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_skintone', [(\"pbm_classes\", [\"unknown skin-tone\", \"light-skinned person\", \"dark-skinned person\"])]),\n",
    "    (lambda k, tol: lambda x: debias_database.search(x, k), 'DebiasClip', [])\n",
    "]\"\"\"\n",
    "\n",
    "\"\"\"method_name_specification_list = [\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_gender', [(\"pbm_classes\", [\"unknown gender\", \"man\", \"woman\"])]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_intersectional', [(\"pbm_classes\", [\"unknown gender and skin-tone\", \"light-skinned man\", \"light-skinned woman\", \"dark-skinned man\", \"dark-skinned woman\"])]),\n",
    "    (lambda k, eps: lambda x: image_database.pbm(x, k, eps=eps), 'PBM_skintone', [(\"pbm_classes\", [\"unknown skin-tone\", \"light-skinned person\", \"dark-skinned person\"])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='feature_distances'), 'CDI_Features', [])\n",
    "]\"\"\"\n",
    "\n",
    "method_name_specification_list = [\n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"gender\", n, k), 'CLIP_gender', []),\n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"skintone\", n, k), 'CLIP_skintone', []),\n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"intersectional\", n, k), 'CLIP_intersectional', []),\n",
    "    (lambda k, n: lambda x: image_database.clip_clip(x, \"three_attr\", n, k), 'CLIP_three_attributes', []),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='max_min'), 'CDI_Min_skintone', [(\"sensitive_attributes\", [(\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_sum'), 'CDI_EucSum_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])]),\n",
    "    (lambda k, tol: lambda x: image_database.distinct_retrival(x, k, tol=tol, method='euc_max_min'), 'CDI_EucMin_intersectional', [(\"sensitive_attributes\", [(\"A picture of a man\", \"A picture of a woman\"), (\"A picture of a light-skinned person\", \"A picture of a dark-skinned person\")])])\n",
    "]\n",
    "\n",
    "ks = [10, 25, 50, 100]\n",
    "\n",
    "number_of_tol_steps = 16\n",
    "number_of_eps_steps = 11\n",
    "random_iters = 4\n",
    "number_of_clip_clip_steps = 24\n",
    "\n",
    "result_dicts = []\n",
    "\n",
    "for method, name, spec in method_name_specification_list:\n",
    "    print(f\"Starting analysis for method: {name}...\")\n",
    "    for s, val in spec:\n",
    "        if s == \"sensitive_attributes\":\n",
    "            image_database.sensitive_attributes(val)\n",
    "        if s == \"pbm_classes\":\n",
    "            image_database.define_pbm_classes(val)\n",
    "    for k in tqdm(ks):\n",
    "        result_dict = {'name': name}\n",
    "\n",
    "        if name in ['Baseline', \"DebiasClip\"]:\n",
    "            steps = 1\n",
    "        else:\n",
    "            steps = number_of_tol_steps\n",
    "\n",
    "        if name[0:3] == 'PBM':\n",
    "            for e in reversed(range(0, number_of_eps_steps)):\n",
    "                eps = e / (number_of_eps_steps - 1)\n",
    "                retrieval_function = method(k, eps)\n",
    "                new_dict = result_dict.copy()\n",
    "                random_results = []\n",
    "                for i in range(random_iters):\n",
    "                    new_dict = result_dict.copy()\n",
    "                    run_analysis(retrieval_function, k, eps, new_dict, catagories, 'search_term', indistinguisable_values,  ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "                    random_results.append(new_dict)\n",
    "                \n",
    "                add_dict = result_dict.copy()\n",
    "                for key in random_results[0].keys():\n",
    "                    if key == 'name':\n",
    "                        continue\n",
    "                    add_dict[key] = np.mean([res[key] for res in random_results], axis=0)\n",
    "                result_dicts.append(add_dict)\n",
    "        \n",
    "        elif name[0:4] == 'CLIP':\n",
    "            for e in range(0, number_of_clip_clip_steps):\n",
    "                n = e * 20\n",
    "                retrieval_function = method(k, n)\n",
    "                new_dict = result_dict.copy()\n",
    "                run_analysis(retrieval_function, k, n, new_dict, catagories, 'search_term', indistinguisable_values, ['image_gender'],  [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "                result_dicts.append(new_dict)\n",
    "\n",
    "        else:\n",
    "            for t in range(0, steps):\n",
    "                if steps == 1:\n",
    "                    tol = None\n",
    "                else:\n",
    "                    tol = t / 200\n",
    "                retrieval_function = method(k, tol)\n",
    "                new_dict = result_dict.copy()\n",
    "                if name == \"CDI_Random\":\n",
    "                    random_results = []\n",
    "                    for i in range(random_iters):\n",
    "                        new_dict = result_dict.copy()\n",
    "                        run_analysis(retrieval_function, k, tol, new_dict, catagories, 'search_term', indistinguisable_values,  ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "                        random_results.append(new_dict)\n",
    "                    new_dict = result_dict.copy()\n",
    "\n",
    "                    for key in random_results[0].keys():\n",
    "                        if key == 'name':\n",
    "                            continue\n",
    "                        new_dict[key] = np.mean([res[key] for res in random_results], axis=0)\n",
    "                else:\n",
    "                    run_analysis(retrieval_function, k, tol, new_dict, catagories, 'search_term', indistinguisable_values, ['image_gender'], [['woman']], [['man']], true_rates, totals_by_cat)\n",
    "                result_dicts.append(new_dict)\n",
    "\n",
    "df = parse_analysis(result_dicts, ['gender'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_pickle(\"results/giis-3-24-CLIP-euc.pkl\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"results/giis-3-21.pkl\")\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_across_tol(df, k, method_names, axis1, axis2, xlabel=None, ylabel=None, title=None, reverse_x = False):\n",
    "    relevant_k = df[df['k'] == k]\n",
    "\n",
    "    for method in method_names:\n",
    "        data_for_method = relevant_k[relevant_k['name'] == method]\n",
    "        d1 = data_for_method[axis1]\n",
    "        d2 = data_for_method[axis2]\n",
    "\n",
    "        if method in [\"Baseline\", \"DebiasClip\"]:\n",
    "            plt.scatter(d1, d2, label=method)\n",
    "        else:\n",
    "            plt.plot(d1, d2, label=method)\n",
    "    plt.legend()\n",
    "    if xlabel is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "    else:\n",
    "        plt.xlabel(axis1)\n",
    "    if ylabel is not None:\n",
    "        plt.ylabel(ylabel)\n",
    "    else:\n",
    "        plt.ylabel(axis2)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    else:\n",
    "        plt.title(f\"Graph of {axis2} over {axis1}\")\n",
    "    if reverse_x:\n",
    "        plt.gca().invert_xaxis()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names = ['Baseline', 'DebiasClip', \"CDI_Sum_gender\"]\n",
    "method_names = df2['name'].unique()\n",
    "\n",
    "for k in [10, 25, 50, 100]:\n",
    "    plot_across_tol(df2, 25, method_names, 'Avg_AbsBias_gender', 'Avg_Precision', reverse_x=True)\n",
    "    plot_across_tol(df2, 25, method_names, 'Avg_Bias_gender', 'Avg_Precision', reverse_x=False)\n",
    "    plot_across_tol(df2, 25, method_names, 'Avg_Skew_gender', 'Avg_Precision', reverse_x=False)\n",
    "    plot_across_tol(df2, 25, method_names, 'Avg_AbsBias_gender', 'Avg_Recall', reverse_x=True)\n",
    "    plot_across_tol(df2, 25, method_names, 'Avg_Bias_gender', 'Avg_Recall', reverse_x=False)\n",
    "    plot_across_tol(df2, 25, method_names, 'Avg_AbsBias_for_Accurate_gender', 'Avg_Precision', reverse_x=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
